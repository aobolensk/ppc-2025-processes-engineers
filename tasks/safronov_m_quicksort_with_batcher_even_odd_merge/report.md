# Быстрая сортировка с четно-нечетным слиянием Бэтчера.

- Student: Сафронов Максим Александрович, group 3823Б1ПР4
- Technology: SEQ | MPI
- Variant: 15

## 1. Introduction
Задача заключается в сортировке входного массива целых чисел по возрастанию с использованием гибридного алгоритма, объединяющего быструю сортировку и чётно-нечётное слияние Бэтчера. В последовательной и параллельной версиях используется алгоритм быстрой сортировки написанный в ручную.
Также выполнено сравнение их производительности.

## 2. Problem Statement
На вход подаётся массив целых чисел размером N.
Требуется отсортировать массив по возрастанию с использованием алгоритма, который сочетает быструю сортировку и чётно-нечётное слияние Бэтчера.
В результате должен получиться отсортированный по возрастанию вектор длиной N.

## 3. Baseline Algorithm (Sequential)
Последовательная версия алгоритма реализована на основе не рекурсивной быстрой сортировки.
Использование чётно-нечётного слияния Бэтчера в последовательной версии не рассматривается, поскольку данный механизм эффективен именно в условиях параллельных вычислений.
Вместо стандартной рекурсии используется стек для хранения диапазонов подмассивов, которые необходимо сортировать.
Основные шаги алгоритма:
1. Инициализируется стек с диапазоном всего массива.
2. Пока стек не пуст, извлекается текущий диапазон [left, right].
3. Если диапазон содержит более одного элемента, выбирается опорный элемент — центральный элемент подмассива.
4. Выполняется разбиение подмассива: элементы меньше опорного сдвигаются влево, больше — вправо.
5. Полученные новые диапазоны (левый и правый подмассивы) помещаются в стек для дальнейшей обработки.
6. Итерации повторяются до тех пор, пока все диапазоны не будут обработаны, что гарантирует полную сортировку массива.
Результат сохраняется в выходной вектор.

## 4. Parallelization Scheme
Параллельная версия алгоритма сочетает быструю сортировку с чётно-нечётным слиянием Бэтчера для упорядочивания локальных массивов между процессами.

Распределение данных: 
1. Пусть имеется массив из N элементов и P процессов MPI.
2. Главный процесс (ранг 0) отвечает за распространение исходного массива и за сбор окончательного результата.
3. Рассылка данных:
    * Сначала главный процесс с помощью MPI_Bcast рассылает размер массива всем процессам.
    * Затем рассылается сам массив целиком.
4. Вычисление локальных интервалов для каждого процесса:
    * Базовое количество элементов на процесс: whole_part = N / P.
    * Остаток: real_part = N % P.
    * Первые real_part процессов получают whole_part + 1 элементов, остальные — whole_part.

5. Главный процесс отправляет каждому процессу (кроме себя) его диапазон [start, end] с помощью MPI_Send.
Остальные процессы принимают свои диапазоны через MPI_Recv.
6. Каждый процесс "вырезает" локальный кусок данных own_data для сортировки.

Локальная сортировка:

Каждый процесс сортирует свой локальный массив с использованием быстрой сортировки, такой же как в последовательной версии.

Чётно-нечётные фазы Бэтчера:

После локальной сортировки происходит согласование границ между процессами с помощью чётно-нечётных фаз:

1. Even Phase (чётная фаза):
    * Процессы с чётным рангом обмениваются данными с соседним правым процессом (neighbor = 1).
    * Процессы с нечётным рангом обмениваются с соседним левым процессом (neighbor = -1).

2. Odd Phase (нечётная фаза)
    * Процессы с нечётным рангом обмениваются с правым соседом (neighbor = 1).
    * Процессы с чётным рангом (кроме 0) обмениваются с левым соседом (neighbor = -1).

Обмен данными между процессами осуществляется с помощью MPI_Sendrecv. Полученные данные сливаются с локальными через функцию MergeAndSplit, оставляя себе либо меньшие элементы (для левого процесса в паре), либо большие элементы (для правого процесса).

Количество фаз вычисляется как:
phases = (N + min_local_block_size - 1) / min_local_block_size
где min_local_block_size — минимальный размер блока элементов среди всех процессов. Такой подход гарантирует, что даже элементы в самых маленьких блоках успеют пройти через все соседние процессы, участвуя в чётно-нечётных обменах Бэтчера. Это обеспечивает корректное расположение всех элементов в глобально отсортированном массиве.

Сбор результатов:
1. Главный процесс собирает отсортированные блоки от всех процессов.
2. Сначала он помещает свой локальный массив в начало выходного вектора GetOutput.
3. Затем поочерёдно принимает данные от других процессов (MPI_Recv) и добавляет их в GetOutput.
4. Остальные процессы отправляют свои отсортированные массивы главному процессу через MPI_Send.

После завершения, главный процесс рассылает всем процессам окончательный отсортированный массив с помощью MPI_Bcast.

## 5. Implementation Details
Основные файлы:
* ops_seq.cpp — последовательная реализация;
* ops_mpi.cpp — параллельная реализация с использованием MPI;
* common.hpp — определения типов данных;

Основные классы: SafronovMQuicksortWithBatcherEvenOddMergeSEQ и SafronovMQuicksortWithBatcherEvenOddMergeMPI.

## 6. Experimental Setup
- Hardware/OS: CPU - Intel Core i5-11400F, 6 ядер/12 потоков; RAM - 16 Gb; ОС - Windows 10 
- Toolchain: MinGW-w64 (g++ 7.3.0, x86_64-posix-seh), build type: Release  
- Environment: PPC_NUM_PROC
- Data: тестовые данные задаются вручную.

## 7. Results and Discussion

### 7.1 Correctness
В функциональных тестах (func) проверялась корректность алгоритма в различных ситуациях:

* Когда количество элементов в массиве меньше количества процессов;
* Когда количество элементов в массиве равно количеству процессов;
* Когда количество элементов в массиве больше количества процессов;
* Когда массив элементов пустой;
* Проверка была выполнена на разном количестве процессов.

В производительных тестах (perf) используется массив из 70000000 элементов, инициализированный в обратном порядке — от большего к меньшему.

### 7.2 Performance
Present time, speedup and efficiency. Example table:

Тесты на 2 процессах:
| Count             |Time seq версия (с)| Time mpi версия (с) | Ускорение | Эффективность |
|-------------------|----------------|----------------|-----------|-----------|
| 100000               | 0.0000       | 0.0034       | 0.00     | N/A          |
| 500000             | 0.0111      | 0.01349      | 0.00    |  N/A          |
| 5000000             | 0.2807       | 0.1423        | 1.97   |  96.5%      |
| 20000000          | 1.1978     |   0.5986     | 2.00    |    100%     |
| 70000000           | 4.0944        | 2.1309         | 1.92     |   96%   |

Тесты на 4 процессах:
| Count             |Time seq версия (с)| Time mpi версия (с) | Ускорение | Эффективность |
|-------------------|----------------|----------------|-----------|-----------|
| 100000               | 0.0000       | 0.0031       | 0.00     | N/A          |
| 500000             | 0.0111       | 0.01196      | 0.00    |  N/A          |
| 5000000             | 0.2807       | 0.1138        | 2.47    |  61.8%      |
| 20000000          | 1.1978     |   0.4806     | 2.49    |    62.3%     |
| 70000000           | 4.0944        | 1.6557         | 2.47     |   61.8%   |

Тесты MPI выполнялись на 2 и 4 процессах.
Массив с Count элементами инициализирован в обратном порядке.

Как видно из результатов, на малых размерах массивов (≤ 500 000 элементов) последовательная версия выполняется практически мгновенно, тогда как MPI-версия показывает худший результат из-за накладных расходов на инициализацию среды и коммуникацию. С увеличением объема данных (от 5 000 000 элементов и выше) параллельная версия начинает значительно опережать последовательную. Неполное достижение идеального линейного ускорения (2x на 2 процессах и 4x на 4 процессах) объясняется расходами на межпроцессорное взаимодействие: на каждой итерации процессам необходимо обмениваться граничными значениями (MPI_Sendrecv) и синхронизировать фазы чётно-нечётного слияния Бэтчера.

## 8. Conclusions
В результате были реализованы последовательная (SEQ) версия алгоритма быстрой сортировки и параллельная (MPI) версия, дополненная чётно-нечётным слиянием Бэтчера для согласования данных между процессами. В последовательной версии использование сети Бэтчера не применяется, так как связанные с ней накладные расходы не дают выигрыша по производительности в однопоточном режиме. Алгоритм чётно-нечётного слияния используется только в параллельной реализации для корректного объединения локально отсортированных блоков.

Из полученных результатов видно, что MPI-версия работает быстрее Seq-версии на больших размерах массива, тогда как на малых размерах эффективнее последовательный алгоритм.

## 9. References
1. Лекции по параллельному программированию
2. Практики по параллельному программированию
