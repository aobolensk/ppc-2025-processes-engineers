# Умножение матрицы на вектор с ленточной горизонтальной схемой

- Студент: Видерман Александра Геннадьевна, группа 3823Б1ПР2
- Технология: SEQ | MPI
- Вариант: Ленточная горизонтальная схема (11 вариант)

## 1. Введение

Задача умножения матрицы на вектор — базовая операция линейной алгебры. В данной работе реализована ленточная горизонтальная схема, где матрица распределяется по строкам между процессами MPI: каждый процесс получает непрерывный блок строк, полностью копирует вектор, вычисляет произведение своих строк на вектор, после чего результаты собираются на главном процессе. Разработаны последовательная (SEQ) и параллельная (MPI) реализации, проведён анализ их производительности и корректности.

## 2. Постановка задачи

**Входные данные:**
- Матрица A размером M×N (M строк, N столбцов) вещественных чисел
- Вектор B размером N элементов

**Выходные данные:**
- Вектор C размером M элементов, где C[i] = Σ(A[i][j] × B[j]) для j=0..N-1

**Ограничения:**
- Поддерживаются пустые матрицы и векторы
- Размеры матрицы и вектора должны быть согласованы (число столбцов матрицы = размер вектора)
- Все матрицы являются плотными (нет разреженности)
- Результат должен быть точным с относительной погрешностью менее 10⁻¹⁰

**Формальное определение:**
```
Для каждой строки i от 0 до M-1:
    C[i] = Σ(A[i][j] * B[j]) для j = 0...N-1
```

## 3. Базовый алгоритм (последовательная версия)

Последовательный алгоритм использует двойной цикл:
Для каждой строки матрицы i от 0 до M-1 инициализировать sum = 0.0
Для каждого столбца j от 0 до N-1:
-  Вычислить sum += A[i][j] * B[j]
-  Записать результат C[i] = sum

**Сложность:** O(M×N) по времени, O(1) дополнительной памяти для вычислений (не считая хранения входных/выходных данных).

## 4. Схема распараллеливания

### 4.1 Ленточная горизонтальная схема

В горизонтальной схеме матрица распределяется по строкам между процессами:
- Каждый процесс получает непрерывный блок строк матрицы
- Вектор полностью копируется на все процессы
- Каждый процесс вычисляет произведение своих строк на вектор
- Результаты собираются на главном процессе

### 4.2 Распределение данных

**Принцип распределения строк:**
```
rows_per_proc = M / total_processes
remaining_rows = M % total_processes

Для каждого процесса rank:
    if rank < remaining_rows:
        my_row_count = rows_per_proc + 1
    else:
        my_row_count = rows_per_proc
```

**Пример для M=10, 3 процессов:**
- Процесс 0: 4 строки (rows 0-3)
- Процесс 1: 3 строки (rows 4-6)
- Процесс 2: 3 строки (rows 7-9)

### 4.3 Коммуникационная схема

1. **Распределение матрицы**: Главный процесс (rank 0) преобразует матрицу в плоский формат и распределяет блоки через `MPI_Scatterv`
2. **Рассылка вектора**: Вектор рассылается всем процессам через `MPI_Bcast`
3. **Локальные вычисления**: Каждый процесс умножает свои строки на вектор
4. **Сбор результатов**: Частичные результаты собираются на главном процессе через `MPI_Gatherv`
5. **Рассылка результата**: Итоговый вектор рассылается всем процессам через `MPI_Bcast`

**Анализ коммуникационной сложности:**
- Передача матрицы: O(M×N/P) данных на процесс
- Передача вектора: O(N) данных всем процессам
- Сбор результатов: O(M/P) данных от каждого процесса
- Общее время: O(M×N/P) вычислений + O(N + M/P) коммуникаций

## 5. Детали реализации

### 5.1 Структура кода

```
viderman_a_strip_matvec_mult/
├── common/include/
│   └── common.hpp                 # Общие типы данных
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp           # Интерфейс MPI реализации
│   └── src/
│       └── ops_mpi.cpp           # Реализация MPI алгоритма
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp           # Интерфейс последовательной реализации
│   └── src/
│       └── ops_seq.cpp           # Реализация последовательного алгоритма
├── tests/
│   ├── functional/
│   │   └── main.cpp              # Функциональные тесты (23 теста)
│   └── performance/
│       └── main.cpp              # Тесты производительности
├── data/                         # Тестовые данные (23 файла)
│   ├── empty.txt
│   ├── 1x1_positive.txt
│   ├── square_2x2_simple.txt
│   ├── ...
├── .clang-tidy
├── info.json
├── report.md
└── settings.json
```

### 5.2 Ключевые классы и функции

**Базовые типы (`common.hpp`):**
```cpp
using InType = std::pair<std::vector<std::vector<double>>, std::vector<double>>;
using OutType = std::vector<double>;
using TestType = std::tuple<std::string, double>;
using BaseTask = ppc::task::Task<InType, OutType>;
```

**Последовательная реализация (`VidermanAStripMatvecMultSEQ`):**
- `ValidationImpl()` — проверка согласованности размеров
- `RunImpl()` — последовательное умножение с двойным циклом

**MPI реализация (`VidermanAStripMatvecMultMPI`):**
- `RunImpl()` — распределённое умножение с MPI коммуникациями
- Использование `MPI_Scatterv`, `MPI_Bcast`, `MPI_Gatherv`

### 5.3 Обработка граничных случаев

1. **Пустые входные данные**: Матрица или вектор могут быть пустыми
2. **Несогласованные размеры**: Проверка в `ValidationImpl()` 
3. **Неравномерное распределение**: Корректная обработка остаточных строк
4. **Один процесс**: Алгоритм работает как последовательный
5. **Больше процессов чем строк**: Некоторые процессы получают 0 строк

### 5.4 Использование памяти

**MPI версия:**
- Главный процесс: O(M×N) для полной матрицы
- Другие процессы: O((M/P)×N) для локальной части матрицы
- Все процессы: O(N) для вектора, O(M/P) для частичных результатов

**Оптимизации:**
- Преобразование матрицы в плоский формат для эффективной передачи
- Минимизация временных копий данных
- Локальные вычисления без дополнительной памяти

## 6. Экспериментальная установка

### 6.1 Аппаратное обеспечение
- **Процессор**: 13th Gen Intel Core i7-13620H (10 ядер, 16 потоков)
- **Оперативная память**: 32 GB DDR4
- **Операционная система**: Windows 10 Pro 64-bit

### 6.2 Инструментальная цепочка
- **Компилятор**: MSVC 19.38.33135.0
- **Стандарт C++**: C++17
- **MPI реализация**: Microsoft MPI v10.1.3
- **Система сборки**: CMake 3.30.8
- **Тип сборки**: Release

### 6.3 Тестовые данные
23 тестовых случая, включая:
- Различные размеры матриц (1×1 до 10×10)
- Различные типы данных (целые, дробные, отрицательные)
- Специальные матрицы (нулевая, единичная, диагональная, пустая)
- Проверка точности и обработки граничных случаев

## 7. Результаты и обсуждение

### 7.1 Корректность

**Методы верификации:**
- 23 функциональных теста с эталонными значениями
- Проверка относительной погрешности (10⁻¹⁰)
- Тестирование граничных случаев (пустые данные, несогласованные размеры)

**Результаты:**
- Все 46 тестов успешно пройдены (23 SEQ + 23 MPI)
- SEQ и MPI реализации дают идентичные результаты
- Корректная обработка специальных случаев:
  - Пустые матрицы и векторы
  - Матрицы разных размеров
  - Числа разных знаков и порядков

### 7.2 Производительность

**Методика измерений:**
- Тестирование на матрицах размером 2000×2000
- Измерение времени выполнения для разного числа процессов
- Расчет ускорения и эффективности

**Результаты производительности:**

| Режим  | Процессы | Время, с | Ускорение (отн. SEQ) | Эффективность |
|--------|----------|----------|----------------------|---------------|
| seq    | 1        | 0.00325  | 1.00                 | N/A           |
| mpi    | 1        | 0.04281  | 0.076                | 7.6%          |
| mpi    | 2        | 0.04125  | 0.079                | 3.9%          |
| mpi    | 4        | 0.04627  | 0.070                | 1.8%          |
| mpi    | 8        | 0.05693  | 0.057                | 0.7%          |
| mpi    | 16       | 0.05959  | 0.055                | 0.3%          |

**Анализ результатов:**

**Особенности производительности для малых данных:**
- Последовательная реализация значительно превосходит MPI для матрицы 2000×2000
- SEQ время: 0.00325 с vs MPI (1 процесс): 0.04281 с
- Увеличение числа процессов MPI не приводит к ускорению, а наоборот увеличивает время

**Динамика масштабируемости MPI:**
- 1→2 процесса: время уменьшается с 0.04281 с до 0.04125 с (незначительное ускорение)
- 2→4 процесса: время увеличивается до 0.04627 с (замедление)
- 4→8 процессов: время увеличивается до 0.05693 с
- 8→16 процессов: время увеличивается до 0.05959 с

**Ограничения для малых данных:**
- Коммуникационные накладные расходы доминируют над вычислительной нагрузкой
- Каждый MPI процесс обрабатывает слишком мало данных (~250 тыс. элементов на 16 процессов)
- Преобразование данных и коммуникационные операции создают значительные издержки

**Анализ узких мест для текущего размера данных:**
1. **Инициализация MPI**: Затраты на запуск и синхронизацию процессов
2. **Преобразование данных**: Конвертация двумерной матрицы в плоский формат
3. **Коммуникационные операции**: `MPI_Scatterv`, `MPI_Bcast`, `MPI_Gatherv` имеют значительную латентность
4. **Неравномерное распределение**: Вычисление смещений и размеров для каждого процесса

## 8. Выводы

### Ключевые результаты:
1. **Корректность**: Реализация корректно обрабатывает все 23 тестовых случая, SEQ и MPI дают идентичные результаты
2. **Производительность для малых данных**: SEQ значительно превосходит MPI (в 13-18 раз)
3. **Масштабируемость**: MPI не демонстрирует преимущества для матрицы 2000×2000

### Практическая значимость:
1. **Для малых матриц** последовательная реализация является оптимальным выбором
2. **Ленточная горизонтальная схема** начнёт демонстрировать преимущество только при значительно больших размерах матриц
3. **Критический размер данных**: MPI становится эффективным при объёме вычислений на процесс, превышающем коммуникационные издержки

### Ограничения и перспективы:
1. **Ограничения**: Текущие тестовые данные (2000×2000) слишком малы для демонстрации преимуществ MPI
2. **Улучшения**: 
   - Увеличение размера тестовых данных до 10000×10000 и более
   - Оптимизация коммуникаций (неблокирующие операции, группировка данных)
   - Использование гибридных подходов (MPI + OpenMP)
3. **Расширения**: 
   - Поддержка разреженных матриц для уменьшения объёма передаваемых данных
   - Использование GPU для вычислений внутри каждого MPI процесса
   - Адаптивное распределение данных в зависимости от размера матрицы

**Итоговый вывод**: Алгоритм корректно реализует ленточную горизонтальную схему умножения матрицы на вектор. Для малых данных (2000×2000) последовательная реализация является оптимальной из-за высоких накладных расходов MPI. Параллельная версия продемонстрирует преимущество при работе с матрицами значительно большего размера, где вычислительная нагрузка превысит коммуникационные издержки.

## 9. Ссылки
1. MPI Forum. MPI: A Message-Passing Interface Standard. https://www.mpi-forum.org
2. Gropp, W., Lusk, E., Skjellum, A. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, 2014.
3. Introduction to Parallel Computing. LLNL. https://computing.llnl.gov/tutorials/parallel_comp/

## Приложение

### Ключевой фрагмент MPI реализации:
```cpp
bool VidermanAStripMatvecMultMPI::RunImpl() {
  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  
  const auto& full_matrix = input.first;
  const auto& full_vector = input.second;
  
  // Определение размеров
  int rows = static_cast<int>(full_matrix.size());
  int cols = static_cast<int>(full_vector.size());
  
  // Распределение строк между процессами
  int rows_per_proc = rows / size;
  int remaining_rows = rows % size;
  int my_row_count = rows_per_proc + (rank < remaining_rows ? 1 : 0);
  
  // Преобразование матрицы в плоский формат
  std::vector<double> flat_full_matrix;
  if (rank == 0) {
    for (const auto& row : full_matrix) {
      flat_full_matrix.insert(flat_full_matrix.end(), row.begin(), row.end());
    }
  }
  
  // Распределение матрицы
  std::vector<double> flat_local_matrix(my_row_count * cols);
  std::vector<int> send_counts(size), displacements(size);
  // ... вычисление send_counts и displacements ...
  
  MPI_Scatterv(flat_full_matrix.data(), send_counts.data(), 
               displacements.data(), MPI_DOUBLE,
               flat_local_matrix.data(), my_row_count * cols,
               MPI_DOUBLE, 0, MPI_COMM_WORLD);
  
  // Рассылка вектора
  std::vector<double> local_vector(cols);
  MPI_Bcast(local_vector.data(), cols, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  
  // Локальные вычисления
  std::vector<double> local_result(my_row_count);
  for (int i = 0; i < my_row_count; ++i) {
    double sum = 0.0;
    for (int j = 0; j < cols; ++j) {
      sum += flat_local_matrix[i * cols + j] * local_vector[j];
    }
    local_result[i] = sum;
  }
  
  // Сбор результатов
  std::vector<double> result(rows);
  MPI_Gatherv(local_result.data(), my_row_count, MPI_DOUBLE,
              result.data(), send_counts.data(), displacements.data(),
              MPI_DOUBLE, 0, MPI_COMM_WORLD);
  
  GetOutput() = result;
  return true;
}
```

### Ключевой фрагмент SEQ реализации:
```cpp
bool VidermanAStripMatvecMultSEQ::RunImpl() {
  const auto& matrix = input.first;
  const auto& vector = input.second;
  auto& result = GetOutput();
  
  result.resize(matrix.size());
  for (size_t i = 0; i < matrix.size(); ++i) {
    double sum = 0.0;
    for (size_t j = 0; j < matrix[i].size(); ++j) {
      sum += matrix[i][j] * vector[j];
    }
    result[i] = sum;
  }
  return true;
}
```