# Повышение контраста изображения

- Студент: Дорогин Вадим Антонович
- Группа: 3823Б1ПР3
- Технология: SEQ | MPI
- Вариант: 23

## 1. Введение

Обработка изображений является важной задачей в области машинной графики и компьютерного зрения. Одной из базовых операций является повышение контраста изображения, которое позволяет улучшить визуальное качество изображения за счет расширения диапазона яркостей пикселей.

Целью данной работы является реализация алгоритма повышения контраста изображения с использованием технологии MPI для параллельной обработки больших объемов данных. Параллелизация позволяет значительно ускорить обработку изображений за счет распределения вычислений между несколькими процессами.

## 2. Постановка задачи

Задача заключается в реализации алгоритма линейного растяжения контраста для изображения, представленного в виде одномерного массива пикселей. Каждый пиксель представлен значением типа uint8_t в диапазоне от 0 до 255.

**Входные данные:**
- Одномерный массив пикселей изображения типа std::vector<uint8_t>
- Размер массива может быть произвольным

**Выходные данные:**
- Одномерный массив обработанных пикселей того же размера
- Значения пикселей нормализованы в диапазон от 0 до 255

**Ограничения:**
- Изображение должно быть непустым
- Если все пиксели имеют одинаковое значение, результат совпадает с исходным изображением
- Алгоритм должен работать корректно на различном количестве процессов MPI

## 3. Последовательный алгоритм 

Последовательный алгоритм повышения контраста основан на методе линейного растяжения гистограммы. Реализация состоит из следующих этапов:

1. **Валидация входных данных:** Проверка, что входной массив не пуст.

2. **Препроцессинг:** Копирование входных данных во внутренний буфер и инициализация результирующего массива.

3. **Основная обработка:**
   - Поиск минимального и максимального значений пикселей в изображении
   - Если минимальное и максимальное значения равны, результат совпадает с исходным изображением
   - Иначе выполняется линейное преобразование каждого пикселя по формуле:
     ```
     result[i] = ((image[i] - min_val) * 255) / (max_val - min_val)
     ```
   - Формула обеспечивает растяжение диапазона значений от [min_val, max_val] до [0, 255]

4. **Постпроцессинг:** Копирование результата в выходной массив.

Временная сложность алгоритма составляет O(n), где n - количество пикселей в изображении. Пространственная сложность также O(n) для хранения входных и выходных данных.

## 4. Схема распараллеливания

Параллельная реализация использует технологию MPI для распределения вычислений между несколькими процессами. Схема распараллеливания основана на декомпозиции данных по процессам.

### 4.1 Распределение данных

Изображение разбивается на части, которые распределяются между процессами. Используется равномерное распределение с учетом остатка от деления:

- Базовый размер части: base = global_size / world_size
- Остаток: rem = global_size % world_size
- Первые rem процессов получают base + 1 элементов, остальные - base элементов

### 4.2 Коммуникационные операции

**Этап 1: Валидация**
- Процесс с рангом 0 проверяет входные данные
- Результат валидации передается всем процессам через MPI_Bcast

**Этап 2: Распределение данных (PreProcessing)**
- Процесс 0 передает размер изображения всем процессам через MPI_Bcast
- Вычисляются массивы counts и displs для неравномерного распределения
- Данные распределяются между процессами с помощью MPI_Scatterv

**Этап 3: Основная обработка (Run)**
- Каждый процесс находит локальные минимум и максимум в своей части данных
- Глобальные минимум и максимум вычисляются с помощью MPI_Allreduce с операциями MPI_MIN и MPI_MAX
- Каждый процесс выполняет линейное преобразование своей части данных
- Результаты собираются на процессе 0 с помощью MPI_Gatherv
- Результат передается всем процессам через MPI_Bcast для обеспечения консистентности

**Этап 4: Постпроцессинг**
- Каждый процесс копирует результат в выходной массив

### 4.3 Топология коммуникаций

Используется топология "звезда" с процессом 0 в качестве корневого узла:
- Процесс 0 выполняет роль координатора 
- Все процессы участвуют в коллективных операциях (Allreduce, Bcast)
- Коммуникации происходят через коммуникатор MPI_COMM_WORLD

## 5. Детали реализации

### 5.1 Структура кода

Иерархия файлов проекта:

- common/include/common.hpp - общие определения типов (InType, OutType)
- seq/include/ops_seq.hpp и seq/src/ops_seq.cpp - последовательная реализация
- mpi/include/ops_mpi.hpp и mpi/src/ops_mpi.cpp - параллельная реализация 
- tests/functional/main.cpp - функциональные тесты
- tests/performance/main.cpp - тесты производительности

### 5.2 Главные классы

**DoroginVContrastEnhancementSEQ:**
- Наследуется от BaseTask<InType, OutType>
- Реализует методы: ValidationImpl, PreProcessingImpl, RunImpl, PostProcessingImpl
- Использует внутренние буферы image_ и result_

**DoroginVContrastEnhancementMPI:**
- Наследуется от BaseTask<InType, OutType>
- Дополнительные поля: world_rank_, world_size_, local_size_, local_image_, result_
- Использует MPI функции для коммуникации

### 5.3 Важные предположения и граничные случаи

- Пустое изображение обрабатывается корректно (валидация возвращает false)
- Изображение с одинаковыми значениями пикселей не изменяется
- Размер изображения может быть не кратен количеству процессов
- Все процессы получают одинаковый результат благодаря MPI_Bcast

### 5.4 Использование памяти

- Последовательная версия: O(n) для входных и выходных данных
- Параллельная версия: O(n/p) для локальных данных каждого процесса, где p - количество процессов
- Дополнительная память для массивов counts и displs: O(p)

## 6. Экспериментальная установка

### 6.1 Аппаратное обеспечение и ОС

- Процессор: 12th Gen Intel(R) Core(TM) i7-1260U (1.10 GHz)
- Количество ядер: 10
- Оперативная память: 16
- Операционная система: 10

### 6.2 Инструментарий

- Компилятор: MSVC
- Тип сборки: Release
- Версия CMake: 4.2.0

### 6.3 Переменные окружения

- PPC_NUM_PROC: количество процессов MPI (1, 2, 4, 6, 8)

### 6.4 Тестовые данные

Тестовые данные генерируются программно в тестах производительности:
- Размер входного массива: 25 000 000 элементов

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверена с помощью набора функциональных тестов, покрывающих различные сценарии:

1. **Базовый градиент** - проверка работы на простом наборе данных
2. **Минимальные и максимальные значения** - граничные случаи
3. **Однородные значения** - случай, когда все пиксели одинаковы
4. **Линейная последовательность** - упорядоченные данные
5. **Один пиксель** - минимальный размер изображения
6. **Граничные значения** - проверка работы с 0 и 255
7. **Большое изображение** - проверка на 1000 пикселях
8. **Малый диапазон** - проверка на данных с небольшим разбросом значений
9. **Смежные минимальные значения** - граничный случай
10. **Смежные максимальные значения** - граничный случай

Все тесты выполняются, что подтверждает корректность параллельной реализации.

### 7.2 Производительность

Результаты измерений производительности представлены в двух таблицах ниже:

#### 7.2.1 Режим task_run

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 0.116    | 1.00      | N/A           |
| mpi   | 1         | 0.101    | 1.15      | 115.0%        |
| mpi   | 2         | 0.055    | 2.11      | 105.5%        |
| mpi   | 4         | 0.039    | 2.96      | 74.0%         |
| mpi   | 6         | 0.038    | 3.01      | 50.2%         |
| mpi   | 8         | 0.033    | 3.52      | 44.0%         |

#### 7.2.2 Режим pipeline

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 0.178    | 1.00      | N/A           |
| mpi   | 1         | 0.105    | 1.69      | 169.0%        |
| mpi   | 2         | 0.067    | 2.66      | 133.0%        |
| mpi   | 4         | 0.043    | 4.10      | 102.5%        |
| mpi   | 6         | 0.048    | 3.71      | 61.8%         |
| mpi   | 8         | 0.057    | 3.12      | 39.0%         |

**Анализ результатов:**

Результаты измерений показывают, что параллельная реализация обеспечивает значительное ускорение по сравнению с последовательной версией в обоих режимах. В режиме run при использовании одного процесса MPI версия показывает ускорение 1.15, что может быть связано с оптимизациями компилятора или особенностями реализации.

В режиме pipeline результаты более впечатляющие: при 4 процессах достигается ускорение 4.10 при эффективности 102.5%, что указывает на почти идеальное масштабирование. Это связано с тем, что в режиме pipeline учитывается время выполнения всех этапов, включая коммуникационные операции, которые также параллелизуются.

При увеличении количества процессов до 2 в режиме run наблюдается почти идеальное ускорение, что указывает на эффективное использование двух процессов и минимальные накладные расходы на коммуникацию. В режиме pipeline при 2 процессах ускорение составляет 2.66 при эффективности 133.0%

При 4 процессах в режиме run ускорение составляет 2.96 при эффективности 74.0%. В режиме pipeline при 4 процессах достигается максимальное ускорение 4.10 при эффективности 102.5%, что является оптимальным результатом.

При дальнейшем увеличении количества процессов в обоих режимах наблюдается снижение эффективности. В режиме run эффективность падает до 50.2% и 44.0% соответственно. В режиме pipeline эффективность снижается еще сильнее - до 61.8% и 39.0%. Это связано с тем, что накладные расходы на коммуникацию начинают преобладать над выигрышем от параллелизма.

**Ускорение:** вычисляется как отношение времени последовательной версии к времени параллельной версии на p процессах.

**Эффективность:** вычисляется как отношение ускорения к количеству процессов, выраженное в процентах.

**Узкие места и ограничения масштабируемости:**

- Коммуникационные операции (Scatterv, Gatherv, Allreduce) создают накладные расходы
- При малом размере данных накладные расходы на коммуникацию могут превышать выигрыш от параллелизма
- Процесс 0 выполняет дополнительную работу по координации, что может создавать дисбаланс нагрузки
- С увеличением количества процессов эффективность может снижаться из-за роста коммуникационных затрат

## 8. Выводы

В ходе выполнения работы была реализована параллельная версия алгоритма повышения контраста изображения с использованием технологии MPI. Последовательная версия использует метод линейного растяжения гистограммы с временной сложностью O(n).

Параллельная реализация основана на декомпозиции данных по процессам с использованием коллективных операций MPI (Scatterv, Gatherv, Allreduce, Bcast). Это позволяет эффективно распределять вычисления между процессами и синхронизировать результаты.

Корректность реализации подтверждена набором функциональных тестов, покрывающих различные граничные случаи. Результаты последовательной и параллельной версий полностью совпадают.

Экспериментальные результаты показывают, что параллельная версия обеспечивает ускорение до 3.52 раза при использовании 8 процессов. Максимальная эффективность (105.5%) достигается при использовании 2 процессов, что указывает на оптимальное соотношение между выигрышем от параллелизма и накладными расходами. При 4 процессах ускорение составляет 2.96 при эффективности 74.0%, что является хорошим компромиссом для данного размера данных. Дальнейшее увеличение количества процессов приводит к снижению эффективности из-за роста коммуникационных затрат.

**Ограничения:**
- Масштабируемость ограничена коммуникационными операциями - при более чем 4 процессах эффективность начинает заметно снижаться
- Требуется синхронизация всех процессов, что может создавать узкие места при большом количестве процессов

## 9. Источники

1. Message Passing Interface Forum. MPI: A Message-Passing Interface Standard, Version 4.0. https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf

2. OpenMPI Documentation. https://www.open-mpi.org/doc/

3. Сысоев А.В., Лекции по параллельному программированию, ННГУ, 2025

## Приложение

### Фрагмент кода: Основная логика последовательного алгоритма

```cpp
bool DoroginVContrastEnhancementSEQ::RunImpl() {
  if (image_.empty()) {
    return true;
  }

  uint8_t min_val = *std::ranges::min_element(image_);
  uint8_t max_val = *std::ranges::max_element(image_);

  if (min_val == max_val) {
    result_ = image_;
    return true;
  }

  int range = max_val - min_val;
  for (std::size_t i = 0; i < image_.size(); ++i) {
    result_[i] = static_cast<uint8_t>(((image_[i] - min_val) * 255) / range);
  }

  return true;
}
```

### Фрагмент кода: Распределение данных в параллельной версии

```cpp
bool DoroginVContrastEnhancementMPI::PreProcessingImpl() {
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank_);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size_);

  int global_size = 0;
  if (world_rank_ == 0) {
    image_ = GetInput();
    global_size = static_cast<int>(image_.size());
  }

  MPI_Bcast(&global_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> counts(world_size_, 0);
  std::vector<int> displs(world_size_, 0);

  int base = global_size / world_size_;
  int rem = global_size % world_size_;
  for (int i = 0; i < world_size_; ++i) {
    counts[i] = base + (i < rem ? 1 : 0);
  }
  for (int i = 1; i < world_size_; ++i) {
    displs[i] = displs[i - 1] + counts[i - 1];
  }

  local_size_ = counts[world_rank_];
  local_image_.resize(local_size_);

  MPI_Scatterv(world_rank_ == 0 ? image_.data() : nullptr, 
               counts.data(), displs.data(), MPI_UNSIGNED_CHAR,
               local_image_.data(), local_size_, MPI_UNSIGNED_CHAR, 0, 
               MPI_COMM_WORLD);

  result_.resize(global_size);
  return true;
}
```

### Фрагмент кода: Вычисление глобальных минимума и максимума

```cpp
bool DoroginVContrastEnhancementMPI::RunImpl() {
  unsigned char local_min = 255;
  unsigned char local_max = 0;

  if (local_size_ > 0) {
    local_min = *std::ranges::min_element(local_image_);
    local_max = *std::ranges::max_element(local_image_);
  }

  unsigned char global_min = 255;
  unsigned char global_max = 0;

  MPI_Allreduce(&local_min, &global_min, 1, MPI_UNSIGNED_CHAR, 
                MPI_MIN, MPI_COMM_WORLD);
  MPI_Allreduce(&local_max, &global_max, 1, MPI_UNSIGNED_CHAR, 
                MPI_MAX, MPI_COMM_WORLD);

  // ... дальнейшая обработка
}
```

