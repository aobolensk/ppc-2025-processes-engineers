## Параллельный метод Гаусса-Жордана с использованием MPI

- Student: <Зязева Светлана Александровна>, group <3823Б1ПР2>
- Technology: <SEQ | MPI>
- Variant: <17>

---

## 1. Introduction

Метод Гаусса-Жордана — это алгоритм линейной алгебры, применяемый для приведения матрицы к диагональному виду и решения систем линейных уравнений. С увеличением размерности матрицы вычислительная сложность алгоритма растёт кубически, что делает последовательную реализацию крайне затратной по времени для больших систем. Параллельные вычисления позволяют распределить эту нагрузку между несколькими вычислительными узлами, что теоретически может привести к значительному ускорению.

---

## 2. Problem Statement

Требуется реализовать алгоритм метода Гаусса-Жордана для приведения расширенной матрицы A размерности N × (N+1) к диагональному виду. Для системы уравнений A'x = b, представленной в виде расширенной матрицы, алгоритм должен выполнить следующие преобразования для каждого ведущего элемента k от 0 до N-1:

- Нормировка строки k: Все элементы строки k делятся на ведущий элемент A[k][k].

- Исключение столбца k из всех других строк: Для каждой строки i (где i ≠ k) выполняется операция: A[i][j] = A[i][j] - A[i][k] * A[k][j] для всех j.

Конечным результатом работы алгоритма является вектор решений x, элементы которого расположены в последнем столбце преобразованной матрицы.

### Ограничения и требования:
- Все вычисления проводятся в арифметике с плавающей точкой.
- Алгоритм должен корректно обрабатывать необходимость перестановки строк при малом значении ведущего элемента.

---

## 3. Baseline Algorithm (Sequential)

Базовый последовательный алгоритм реализует описанные выше шаги в виде цикла по ведущим строкам. 

```cpp
namespace {

bool FindAndSwapPivotRow(std::vector<std::vector<float>>& a, int i, int n, float epsilon) {
  if (std::abs(a[i][i]) < epsilon) {
    int c = 1;
    while ((i + c) < n && std::abs(a[i + c][i]) < epsilon) {
      c++;
    }

    if ((i + c) == n) {
      return false;
    }

    for (int k = 0; k <= n; k++) {
      std::swap(a[i][k], a[i + c][k]);
    }
  }
  return true;
}

void NormalizeCurrentRow(std::vector<std::vector<float>>& a, int i, int n) {
  float pivot = a[i][i];
  for (int k = i; k <= n; k++) {
    a[i][k] /= pivot;
  }
}

void EliminateColumn(std::vector<std::vector<float>>& a, int i, int n) {
  for (int j = 0; j < n; j++) {
    if (j != i) {
      float factor = a[j][i];
      for (int k = i; k <= n; k++) {
        a[j][k] -= factor * a[i][k];
      }
    }
  }
}

std::vector<float> ExtractSolutions(const std::vector<std::vector<float>>& a, int n) {
  std::vector<float> solutions(n);
  for (int i = 0; i < n; i++) {
    solutions[i] = a[i][n];
  }
  return solutions;
}

}  // namespace

bool ZyazevaSGaussJordanElSEQ::RunImpl() {
  constexpr float kEpsilon = 1e-7F;

  std::vector<std::vector<float>> a = GetInput();
  int n = static_cast<int>(a.size());

  for (int i = 0; i < n; i++) {
    if (!FindAndSwapPivotRow(a, i, n, kEpsilon)) {
      GetOutput() = std::vector<float>();
      return false;
    }

    NormalizeCurrentRow(a, i, n);
    EliminateColumn(a, i, n);
  }

  std::vector<float> solutions = ExtractSolutions(a, n);
  GetOutput() = solutions;

  return true;
}
```
---

## 4. Parallelization Scheme

Параллельный алгоритм вычисления скалярного произведения реализован следующим образом:

1. **Инициализация MPI:** — Каждому процессу присваивается уникальный ранг `rank` и выясняется общее количество процессов `size`.
2. **Разделение данных:**  
   - Распределение данных (MPI_Scatterv): Процесс 0 делит строки матрицы между всеми процессами. 
3. **Цикл по столбцам:**  
   - Выбор ведущего элемента: Все процессы ищут максимальный элемент в столбце k среди своих строк. Глобальный максимум находится с помощью MPI_Allreduce.
   - Перестановка строк: При необходимости строка с максимумом меняется местами со строкой k (локально или через MPI_Send/MPI_Recv).
   - Нормировка и рассылка (MPI_Bcast): Процесс-владелец строки k нормирует её и рассылает остальным.
   - Исключение столбца: Все процессы вычитают нормированную строку из своих строк. 
4. **Сбор результатов (MPI_Gatherv):**  
   Решения собираются на процессе 0.
5. **Итог: Вывод результата и завершение работы MPI.**

---

## 5. Experimental Setup

**Тестовое окружение**
- Процессор и операционная система: AMD Ryzen 7 8845HS, 4 ядра, 32 GB RAM, Windows 11 pro x64
- Инструменты:
    - Cmake 3.28.3
    - Компилятор: g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
    - Использовался Docker-контейнер.
- Данные: Для замера производительности использовались матрицы размером 500 * 501, 1000 * 1001, 2000 * 2000.

---

## 6. Results and Discussion

### Размер матрицы \(500 * 501\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 0.9194 | 1 |
| MPI pipeline | 0.6331 | 4 | 
| MPI pipeline | 0.3381 | 8 | 
| SEQ task_run | 0.8640 | 1 | 
| MPI task_run | 0.6862 | 4 | 
| MPI task_run | 0.3670 | 8 | 

MPI быстрее SEQ в 1.3-1.5 раз при запуске на 4 процессах, в 2.4-2.7 раз при запуске на 8.

### Размер матрицы \(1000 * 1001\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 0.8493 | 1 |
| MPI pipeline | 0.5861 | 4 |
| MPI pipeline | 0.3906 | 8 |  
| SEQ task_run | 1.0394 | 1 | 
| MPI task_run | 0.6169 | 4 | 
| MPI task_run | 0.3504 | 8 | 

MPI быстрее SEQ в в 1.5–1.7 раз при запуске на 4 процессах, в 2.2–3.0 раза при запуске на 8.

### Размер матрицы \(2000 * 2001\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 1.8347 | 1 |
| MPI pipeline | 0.7715 | 4 |
| MPI pipeline | 0.5626 | 8 |  
| SEQ task_run | 1.9019 | 1 | 
| MPI task_run | 0.7703 | 4 | 
| MPI task_run | 0.5714 | 8 | 

MPI быстрее SEQ в в 2.3-2.4 раз при запуске на 4 процессах, в 3.2–3.3 раза при запуске на 8.

**Вывод:** Параллельная реализация успешно решает поставленную задачу — обеспечивает ускорение более чем в 3 раза при обработке крупных матриц, что подтверждает целесообразность использования MPI для распределённых вычислений в линейной алгебре.

---

## 7. Conclusions
1. Созданы две реализации алгоритма: последовательная и с использованием MPI.  
2. При увеличении объёмов данных MPI-версия начинает работать быстрее в 2-3 раза по сравнению с последовательной версией.

---

## 8. References

1. Курс лекций "Параллельное программирование для кластерных систем"
2. Функции MPI. - https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions
3. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru
