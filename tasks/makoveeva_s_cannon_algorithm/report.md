# Умножение плотных матриц (SEQ + MPI). Алгоритм Кэннона

- **Студент:** Маковеева Софья  
- **Группа:** 3823Б1ПР1  
- **Технология:** SEQ + MPI  
- **Вариант:** 1  

---

## 1. Введение

В данной работе рассматривается задача умножения плотных квадратных матриц с элементами типа `double`.
Цель работы — реализовать последовательную и параллельную версии алгоритма умножения матриц, а также
исследовать эффективность параллельного решения с использованием технологии MPI.

Для параллельной версии используется алгоритм Кэннона, ориентированный на распределённые вычисления
в двумерной решётке процессов. Основное внимание уделяется корректности работы алгоритма, обработке
ограничений по размеру матриц и числу процессов, а также анализу производительности.

---

## 2. Постановка задачи

**Входные данные:**
- две квадратные матрицы размера `N × N`, представленные в виде одномерных массивов;
- целое число `N` — размер матриц.

**Выходные данные:**
- результирующая матрица произведения входных матриц.

**Ограничения:**
- `N > 0`;
- размер входных массивов должен соответствовать `N × N`;
- для MPI-версии количество процессов и размер матрицы должны допускать корректную декомпозицию данных.

---

## 3. Базовый алгоритм (последовательная версия)

В последовательной версии используется классический алгоритм умножения матриц с тремя вложенными циклами.
Для каждой строки первой матрицы и каждого столбца второй матрицы вычисляется скалярное произведение,
результат которого записывается в соответствующую ячейку результирующей матрицы.

Данная версия используется:
- как эталон для проверки корректности;
- как базовая точка отсчёта при анализе ускорения параллельного алгоритма.

---

## 4. Схема распараллеливания (MPI)

### Общая идея

В MPI-версии используется алгоритм Кэннона. Все процессы логически организуются в квадратную двумерную
решётку. Каждому процессу сопоставляется один блок матрицы `A` и один блок матрицы `B`.

Каждый процесс:
- хранит только свой локальный блок данных;
- выполняет локальное умножение блоков;
- участвует в циклическом обмене блоками с соседними процессами.

### Распределение данных

Матрицы разбиваются на равные квадратные блоки. Каждый процесс получает один блок матрицы `A` и один
блок матрицы `B`. Размер блока определяется количеством процессов в строке и столбце решётки.

Перед началом основного цикла выполняется начальный сдвиг блоков:
- блоки матрицы `A` сдвигаются влево на количество позиций, равное номеру строки процесса;
- блоки матрицы `B` сдвигаются вверх на количество позиций, равное номеру столбца процесса.

---

## 5. Обработка ограничений по размеру матриц и числу процессов

Алгоритм Кэннона накладывает строгие требования:
- количество используемых процессов должно образовывать полный квадрат;
- размер матрицы должен делиться на размер решётки процессов без остатка.

В реализации используется следующий подход:
- определяется максимальное возможное квадратное количество процессов, которое не превышает общее число запущенных процессов и при этом корректно делит размер матрицы;
- если процессов запущено больше, чем требуется, используется только их часть;
- неиспользуемые процессы исключаются из вычислений через отдельный MPI-коммуникатор;
- если подходящая конфигурация невозможна, задача корректно завершается на этапе валидации.

Такой подход позволяет запускать программу с произвольным числом процессов без падений и некорректных результатов.

---

## 6. Детали реализации

- Код разделён на логические вспомогательные функции:
  - выбор конфигурации решётки процессов;
  - создание активного и декартового коммуникаторов;
  - рассылка блоков матриц;
  - выполнение основного цикла алгоритма Кэннона;
  - сборка результата и рассылка его всем процессам.
- Используются пользовательские MPI-типы данных для корректной передачи двумерных блоков.
- После завершения вычислений результирующая матрица рассылается всем процессам, чтобы обеспечить единое состояние вывода.

---

## 7. Экспериментальная установка

- **Размер матрицы:** `N = 729`
- **Тип данных:** `double`
- **ОС:** Windows
- **MPI:** Microsoft MPI
- **Сборка:** Release
- **Запуск:** `mpiexec -n <P>`

Измерения проводились для разных количеств MPI-процессов.

---

## 8. Результаты и обсуждение

### 8.1 Корректность

Корректность реализации проверялась путём сравнения результата MPI-версии с эталонной последовательной
версией. Для всех тестовых конфигураций результаты совпадают с допустимой погрешностью вычислений
для чисел с плавающей точкой.

---

### 8.2 Производительность

Результаты измерений времени выполнения MPI-версии приведены в таблице ниже.

| Количество процессов | Время (pipeline), сек | Время (task_run), сек |
|----------------------|-----------------------|-----------------------|
| 1                    | 0.1424                | 0.1403
| 2                    | 0.1253                | 0.1285                |
| 5                    | 0.1268                | 0.1261                |
| 8                    | 0.1278                | 0.1244                |
| 9                    | 0.0361                | 0.0375                |
| 10                   | 0.0381                | 0.0409                |

### Анализ результатов

При увеличении числа процессов наблюдается заметное снижение времени выполнения.
Наибольший выигрыш достигается при использовании 9 и 10 процессов.

Это объясняется тем, что:
- при 9 процессах формируется идеальная квадратная решётка `3 × 3`, полностью соответствующая алгоритму Кэннона;
- данные распределяются равномерно, а коммуникационные издержки минимальны;
- при 10 процессах фактически используется та же конфигурация из 9 активных процессов, но 10-й процесс добавляет накладных расходов за счет того, что он создается, инициализируется, синхронизируется, но ничего не считает

Для малых чисел процессов ускорение ограничено:
- высокой долей последовательных операций;
- затратами на инициализацию MPI и обмен данными.

В целом результаты подтверждают хорошую масштабируемость реализации при корректном выборе конфигурации процессов.

---

## 9. Заключение

В ходе работы была реализована MPI-версия умножения плотных матриц на основе алгоритма Кэннона.
Реализация корректно обрабатывает ограничения по размеру матриц и количеству процессов, обеспечивая
устойчивую работу при различных конфигурациях запуска.

Экспериментальные результаты показывают существенное ускорение по сравнению с последовательной версией
и подтверждают эффективность выбранной схемы распараллеливания.

---

