# Нахождение минимальных значений по столбцам матрицы

**Студент:** Налитов Денис Олегович, группа 3823Б1ПР1.
**Технология:** MPI, SEQ.
**Вариант:** 18.

## 1. Введение

Нахождение минимальных значений по столбцам матрицы — задача, распространённая в машинном обучении, физических и экономических вычислениях. Чтобы достигнуть производительности при работе с большими матрицами, предпочтение отдают параллельным, не последовательным алгоритмам.

Цель работы — сравнение последовательного (SEQ) и параллельного (MPI) алгоритма нахождения минимальных значений столбцов матрицы произвольного размера; оценка эффективности и производительности.

## 2. Постановка задачи

Задача алгоритма заключается в том, чтобы для каждого столбца квадратной матрицы найти его минимальный элемент; из найденных элементов составить вектор.

**Вход:** вектор векторов значений типа int размером `n×n`, `n` > 0.
**Выход:** вектор значений типа int размером `n`.

**Какая матрица используется в тестах.** Тестовые данные создаются детерменированно и с равномерным распределением. Для этого использован быстрый генератор псевдослучайных чисел Xorshift. Диапазон — [-10⁶, 10⁶]. См. алгоритм генерации в разделе «Окружение → Данные».

**Пример ожидаемой работы алгоритма — и последовательного, и параллельного — в ходе решения задачи.** С использованием матрицы 5×5:

```
[  7,  12,  -3,   8,  15 ]
[ -2,   9,   4,  -1,  11 ]
[  5,  -8,   6,  10,  -4 ]
[  3,   0,  -9,   2,   7 ]
[ -6,   5,   1,  -5,  13 ]
```

Найденные минимумы в столбцах: `[ -6, -8, -9, -5, -4 ]`

Вектор-результат: `vector<int> { -6, -8, -9, -5, -4 }`

**Различия в реализации алгоритмов.** Параллельный алгоритм должен быть реализован на технологии MPI. Выходные вектора алгоритмов при этом должны быть одинаковы.

## 3. Последовательный алгоритм

**Временная сложность:** `O(n²)`.
**Потребление памяти при хранении результата:** `O(n)`.

Алгоритм устойчив. При одинаковых `n` результат детерменирован. Пропускная способность ограничена производительностью генератора чисел.

Этапы:

### 1. `ValidationImpl`. Валидация данных на входе

Проверки на этом этапе: а) входное значение `n` должно быть больше нуля; б) выходной вектор должен быть пуст:

```cpp
(GetInput() > 0) && (GetOutput().empty());
```

### 2. `PreProcessingImpl`. Предварительная обработка

На этом этапе очищен выходной вектор и зарезервирована память на `n` элементов:

```cpp
GetOutput().clear();
GetOutput().reserve(GetInput());
```

### 3. `RunImpl`. Имплементация

Алгоритм находит минимальные значения по столбцам матрицы `n×n`. Работает по паттерну Map → Reduce. Генерация данных псевдослучайна.

Особенность имплементации заключается в том, что данные генерируются на лету. Матрица не хранится полностью, что позволяет экономить O(n²) памяти.

Этапы:

**1. Внешний цикл по столбцам:** для каждого столбца инициализировано стартовое значения минимума из первой строки: `generate(0, j)`.

**2. Внутренний цикл по строкам:** для каждой строки сгенерировано значение ячейки: `generate(i, j)`. Текущее значение сравнивается с найденным минимумом с помощью `std::min`.

**3. Сохранение.** После обработки всех строк столбца найденный минимум добавлен в выходной вектор: `GetOutput().push_back(min_val)`.

### 4. `PostProcessingImpl`. Постобработка

На выходе проверяется, что размер полученного вектора — `n`:

```cpp
return !GetOutput().empty() && (GetOutput().size() == static_cast<size_t>(GetInput()));
```

## 4. Схема распараллеливания

В этой секции описано, как последовательный алгоритм переводится в параллельный на MPI: декомпозиция данных, роли процессов, гарантии корректности и расчёт сложности.

### Идея распараллеливания

Матрица размером `n×n` декомпозируется по строкам. Каждый MPI-процесс получает подмножество строк (не все строки каждого столбца). Каждый процесс локально вычисляет минимум по каждому столбцу по своим строкам. Затем результаты по столбцам сводятся (reduce) между процессами операцией `min`. Результат — глобальный минимум для каждого столбца.

Преимущества реализации заключаются в том, что а) генерация значений по индексам так же детерменирована между процессами; б) хранится не вся матрица, поэтому затраты памяти — O(n) на процесс.

### Разбиение данных между процессами

Разбиение строк выполняется равномерно с распределением остатка. Здесь `size` — число MPI-процессов, `rank` — номер процесса (0..size-1):

```cpp
rows_per_process = n / size
leftover = n % size
process_rows_number = rows_per_process + (rank < leftover ? 1 : 0)

first_row = rank * rows_per_process + min(rank, leftover)
last_row = first_row + process_rows_number  // [first_row, last_row)
```

Каждый процесс обрабатывает строки `i ∈ [first_row, last_row)` и для каждой строки обновляет локальные минимумы по всем столбцам `j ∈ [0, n)`.

Это гарантирует: а) ровное распределение нагрузки (разница в количестве строк между процессами ≤ 1); б) детерминированность (каждая клетка обрабатывается точно одним процессом).

### Локальная стадия

На каждом процессе выделяется вектор `local_min_columns` длины `n`, инициализированный `+∞` (или `std::numeric_limits<InType>::max()`).

Псевдокод локальной стадии:

```python
local_min_columns[0..n-1] := +∞

for i in first_row .. last_row-1:
  for j in 0 .. n-1:
    val := Generate(i, j)
    local_min_columns[j] := min(local_min_columns[j], val)
```

### Стадия сведения, Allreduce

После локальной обработки алгоритм получает глобальные минимумы по каждому столбцу. В `ops_mpi` используется:

```cpp
MPI_Allreduce(local_min_columns.data(), global_min_columns.data(), n, MPI_INT, MPI_MIN, MPI_COMM_WORLD);
```

### Сложность, пропускная способность и память

**Локальная временная сложность на каждый процесс:** `O(n * local_rows) ≈ O(n * (n/size)) = O(n² / size)`.

**Стоимость коммуникации:** одна коллективная операция `Allreduce` над векторами длины `n`. При реализации `MPI_Allreduce` приблизительная стоимость — `O(n · log(size))` в количестве сообщений. Это будет зависеть от реализации MPI.

**Память:** `O(n)` на процесс. Затраты на вектор локальных минимумов и вектор результата.

**Общая скорость и ускорение** зависит от соотношения вычислительной работы и стоимости `Allreduce`. Для больших `n` вычисления доминируют и масштабирование хорошее; для малых `n` коммуникация может стать узким местом.

### Роли процессов в зависимости от их `rank`

**Все процессы:** генерируют свои строки и обновляют `local_min_columns`. Роль одинакова для всех процессов.

**Ведущий (корневой, нулевой) процесс.** Если используется `MPI_Allreduce`, все процессы получают финальный вектор и могут проводить постобработку независимо. Если использовать `MPI_Reduce`, то ведущий процесс собирает итоговые минимумы и отвечает за вывод и постобработку.

### Псевдокод MPI-реализации

```pascal
MPI_Init()
rank := MPI_Comm_rank()
size := MPI_Comm_size()

compute first_row, last_row for this rank

local_min[0..n-1] := +∞
for i in first_row .. last_row-1:
  for j in 0 .. n-1:
    local_min[j] := min(local_min[j], Generate(i,j))

global_min[0..n-1] := +∞
MPI_Allreduce(local_min, global_min, n, MPI_MIN, MPI_COMM_WORLD)

store global_min in GetOutput()
MPI_Finalize()
```

### Схема построчного разбиения

```
           ┌────────────┬────────────┬────────────┐
           │   rank 0   │   rank 1   │   rank 2   │
           ├────────────┼────────────┼────────────┤
 rows      │ r0...rA    │ rA...rB    │ rB...rC    │
           └────────────┴────────────┴────────────┘
                    │           │           │
                    ▼           ▼           ▼
           local_min0   local_min1   local_min2
                    └───────┬────────────┘
                            ▼
                  MPI_Allreduce(MIN)
                            ▼
             глобальный вектор: min[j], j ∈ [0; n)
```

## 5. Детали реализации

### Файловая структура

```
nalitov_d_matrix_min_by_columns/
├── common/
│   └── include/
│       └── common.hpp           # Общие типы
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp          # Интерфейс последовательной реализации
│   └── src/
│       └── ops_seq.cpp          # Реализация последовательного алгоритма
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp          # Интерфейс MPI реализации
│   └── src/
│       └── ops_mpi.cpp          # Реализация параллельного алгоритма
└── tests/
    ├── functional/
    │   └── main.cpp             # Функциональные тесты
    └── performance/
        └── main.cpp             # Тесты производительности
```

### Классы и функции

**`BaseTask`** — базовый абстрактный класс, определяющий жизненный цикл задачи.

**`NalitovDMinMatrixSEQ`** — класс последовательной реализации. Его методы: проверка входных данных `ValidationImpl()`, очищение выходного вектора и подготовка памяти `PreProcessingImpl()`, нахождение минимумов `RunImpl()`, проверка размера полученного вектора `PostProcessingImpl()`.

**`NalitovDMinMatrixMPI`** — класс MPI-реализации с аналогичным интерфейсом.

**`Generate(i, j)`** — детерминированная функция генерации элементов матрицы. В основе — Xorshift. Используется в имплементации и в тестах.

**`CalculateExpectedColumnMins(n)`** — утилита для вычисления ожидаемого результата в тестах.

### Граничные случаи

**Соответствие MPI типам.** `InType` = `int`. В `MPI_Allreduce` используется `MPI_INT`. Это корректно при предположении, что `sizeof(int)` и представление значения соответствуют ожиданиям. Если `InType` будет изменён (например, `int32_t`/`int64_t`), нужно согласовать и MPI-тип (`MPI_INT32_T`, `MPI_LONG_LONG`).

**Вход `n == 0`.** `ValidationImpl()` возвращает `false` для `n <= 0`. `RunImpl()` дополнительно защищён проверкой `if (n == 0) return false;`. Тесты проверяют корректный отказ при нулевом входе.

**Процессов больше, чем строк.** Логика `rows_per_process = n / size` + `leftover = n % size` корректно даёт `process_rows_number == 0` для некоторых процессов; такие процессы ничего не обрабатывают и остаются с `local_min_columns` = `+∞`. `MPI_Allreduce` корректно сведёт значения — итоговые минимумы будут вычислены другими процессами. Это допустимо, но при `size >> n` есть потеря эффективности из-а пустых процессов.

**MPI-окружение.** MPI-функции вызываются без дополнительной проверки ошибок. В продакшене необходимо обрабатывать возвращаемые MPI статусы.

**Повторный вызов задачи (reuse).** В тестах проверяется повторное использование объекта `Task`: Run вызывается несколько раз с обновлённым `GetInput()`. `PreProcessingImpl()` очищает и `reserve` результирующий вектор — этого достаточно для многократных запусков.

### Использование памяти

**SEQ:** Основной расход памяти — итоговый вектор `GetOutput()` длины `n`, это `O(n)` памяти. Матрица не хранится.

**MPI (каждый процесс):** `local_min_columns` — вектор длины `n` (инициализирован `+∞`). `GetOutput()` — также вектор длины `n` (результат Allreduce). Следовательно, пиковая память на процессе — ≈ `2 * n * sizeof(InType)` плюс буферы MPI. Итого — `O(n)` на процесс.

**Пространство для улучшения.** Если результат нужен только в `root`, удобно заменить `MPI_Allreduce` на `MPI_Reduce`. Экономия памяти — `n * sizeof(InType)`.

Если память ограничена, будет полезно обработать столбцы блоками определённого размера. Например, по 1000 столбцов за раз: на каждой итерации обработать только столбцы `j ∈ [b, b+k)`, выполнить `Allreduce` по этому блоку, сохранить результаты, перейти к следующему блоку. Это уменьшит пиковую память до `O(k)`.

## 6. Окружение

### Аппаратное обеспечение и ОС

**Процессор:** Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz.
**Оперативная память:** 24 GB, 2133 MHz.
**ОС:** Windows 10 Enterprise LTSC 21H2, Ubuntu 24.04.

### Инструменты

**Компилятор:** MSVC v143 (v.14.44—17.14)
**MPI:** Microsoft MPI v10.1.3 (64-bit)
**Тип сборки:** Release

### Переменные окружения

- `PPC_NUM_PROC`: 1, 2, 4 — задано при запуске `mpiexec -n {N}`.
- `PPC_NUM_THREADS`: 1 для последовательного алгоритма, 4 для параллельного.
- `PPC_ASAN_RUN`, `PPC_IGNORE_TEST_TIME_LIMIT`, `PPC_TASK_MAX_TIME`, `PPC_PERF_MAX_TIME` — заданы по умолчанию.

### Данные

Тестовые данные создаются детерменированно и с равномерным распределением. Для этого использован быстрый генератор псевдослучайных чисел Xorshift. Реализация:

```cpp
uint64_t seed = (i * 100000007ULL + j * 1000000009ULL) ^ 42ULL;

seed ^= seed >> 12;
seed ^= seed << 25;
seed ^= seed >> 27;
uint64_t value = seed * 0x2545F4914F6CDD1DULL;

return static_cast<InType>((value % 2000001) - 1000000);
```

## 7. Результаты и анализ

### 7.1. Корректность

Проверена корректность работы последовательной и параллельной реализаций. Что делают тесты:

**Сравнение результатов SEQ и MPI в функциональных тестах.** Для каждого `n` вычислены минимумы по столбцам обеими версиями алгоритма. Результаты сравнивались поэлементно. Во всех протестированных случаях векторы полностью совпадают, что подтверждает корректность распараллеливания и отсутствия гонок данных.

Работа алгоритма проверена при 11 размерах матриц — до 512×512. В набор входят чётные, нечётные и простые числа. См. `tests/functional/main.cpp`.

**Проверка краевых случаев.** Проверены крайние и граничные значения входа (n = 1, n = 2, матрицы с остатком строк при разбиении по процессам). Проверено поведение при неравномерном числе строк на процесс (n % size ≠ 0). Проверено повторное выполнение одного экземпляра задачи с разнными данными на входе.

**Проверка валидации.** Проверена корректость валидации входных данных: верно ли алгоритм отклоняет n = 0; проходят ли все этапы пайплайна при n > 0.

Тесты пройдены для обеих реализаций при 1, 2, 4 процессах MPI.

### 7.2. Производительность

Тесты производительности проведены в двух режимах: `task_run` — для замера самих вычислений — и `pipeline` — для замера полного времени выполнения пайплайна.

Матрица в тестах производительности имеет размер 10 000 × 10 000.

### Результаты замеров

**seq_enabled:task_run**
Время: 0.2836505600 с · Ускорение: 1.00 · Эффективность: НД

**mpi_enabled:task_run, 2 процесса**
Время: 0.1347278600 с · Ускорение: 2.105 · Эффективность: 105.25%

**mpi_enabled:task_run, 4 процесса**
Время: 0.0781807400 с · Ускорение: 3.628 · Эффективность: 90.70%

**seq_enabled:pipeline**
0.2929174000 с · Ускорение: 1.00 · Эффективность: НД

**mpi_enabled:pipeline, 2 процесса**
Время: 0.1352892200 · Ускорение: 2.165 · Эффективность: 108.25%

**mpi_enabled:pipeline, 4 процесса**
Время: 0.0796599800 с · Ускорение: 3.676 · Эффективность: 91.90%

Формула ускорения: `S(p) = время выполнения последовательной версии T(1) / время выполнения параллельной версии T(p)`
Формула эффективности: `E(p) = S(p) / p × 100%`, где `p` — количество процессов

Тестирование при 8 процессах на четырёхпоточном процессоре Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz приводит к ожидаемому ухудшению производительности по сравнению с запуском на 4 процессах, поскольку процессы начинают делить контекст.

### Ограничения и узкие места

**Генерация.** Алгоритм Xorshift быстрый, но составляет значительную часть вычислительной нагрузки. При увеличении числа процессов генерация может стать узким местом.

**Коммуникации:** Операция `MPI_Allreduce` имеет сложность O(n). При очень больших `n` может стать доминирующей.

## 8. Заключение

### Результаты

**Реализация корректна.** MPI- и SEQ-реализации протестированы на всех наборах тестовых данных. Ошибок при распараллеливании нет, поскольку параллельной реализации идентичны результатам последовательной.

**Распараллеливание с `MPI_Allreduce` эффективно.** MPI показала эффективность 99%—105% при двух и 88%—90% при четырёх процессах. Затраты на инициализацию MPI в конвейере минимальны.

### Технические достижения

**Экономия памяти.** Данные генерируются «на лету», матрица не хранится целиком. Это экономит память: O(n) вместо O(n²).

**Гибкость архитектуры.** Решение с паттерном Map-Reduce возможно адаптировать для задач поиска максимума, суммы, среднего.

**Баланс нагрузки.** Алгоритм распределения остатков строк обеспечивает равномерные вычисления между процессами даже при некратном отношении размера матрицы к количеству процессов.

## 9. Источники

1. Параллельные вычисления. Технологии и численные методы. Учебное пособие в 4 томах. // Гергель В.П., Баркалов К.А., Мееров И.Б., Сысоев А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2013. — 1394 с.

2. Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. // Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.

3. Современные языки и технологии параллельного программирования. // Гергель В.П., — М.: Издательство Московского университета, 2012. — 408 с.

4. Справочник по MPI. Microsoft MPI. // URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-allreduce-function. Дата обращения: 21.11.2025.
