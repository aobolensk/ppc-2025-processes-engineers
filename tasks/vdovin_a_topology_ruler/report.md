# Топология линейка

-  **Студент** Вдовин Артём Николаевич
-  **Группа** 3823Б1ПР4
-  **Технология** SEQ|MPI
-  **Вариант** 6

## 1. Введение

В параллельном программировании важную роль играет организация коммуникаций между процессами. Топология сети определяет структуру связей между процессами и влияет на эффективность передачи данных. В данной работе реализуется виртуальная топология "линейка" с использованием MPI без применения стандартных функций MPI_Cart_Create и MPI_Graph_Create.

Цель данной работы: создать механизм передачи данных от любого процесса любому другому процессу в рамках линейной топологии.

## 2. Постановка задачи

Задача заключается в реализации виртуальной топологии "линейка" для N процессов, где каждый процесс связан только с двумя соседями (кроме крайних процессов, которые имеют по одному соседу). Процессы пронумерованы от 0 до N-1, где процесс i связан с процессами i-1 и i+1.

Входные данные: вектор целых чисел, который передается от процесса 0 остальным процессам.
Выходные данные: тот же вектор, полученный всеми процессами.

## 3. Базовый алгоритм 

Последовательный алгоритм представляет собой простую заглушку, которая копирует входные данные в выходные без какой-либо обработки. Это необходимо для обеспечения совместимости с параллельной версией и корректной работы тестов.

Алгоритм:
1. Получение входных данных
2. Копирование входных данных в выходные
3. Возврат результата

Данный подход гарантирует, что на всех процессах будет одинаковое значение, что критично для корректной работы тестовой инфраструктуры.

## 4. Параллельный алгоритм

Параллельный алгоритм реализует топологию "линейка" для передачи данных от процесса 0 всем остальным процессам. Алгоритм работает следующим образом:

1. Процесс 0 получает входные данные и определяет их размер
2. Размер данных передается всем процессам через MPI_Bcast
3. Каждый процесс (кроме 0) выделяет память для приема данных
4. Данные передаются последовательно по цепочке: процесс 0 -> процесс 1 -> процесс 2 -> ... -> процесс N-1
5. Каждый процесс, получивший данные, передает их следующему процессу 
6. Все процессы получают одинаковые данные

Особенности реализации:
- При одном процессе данные просто копируются без передачи
- Процесс 0 отправляет данные процессу 1
- Процессы с рангом от 1 до N-2 получают данные от левого соседа и отправляют правому
- Процесс N-1 только получает данные, не отправляя дальше

## 5. Схема распараллеливания

Для реализации топологии "линейка" используется следующая схема распараллеливания:

### Топология коммуникаций

Процессы организованы в линейную цепочку:

P0 <-> P1 <-> P2 <-> ... <-> P(N-1)

Каждый процесс знает своих соседей:
- Левый сосед: rank - 1 (если rank > 0)
- Правый сосед: rank + 1 (если rank < size - 1)

### Паттерн обмена данными

1. **Распространение размера данных**: MPI_Bcast от процесса 0 всем процессам
2. **Последовательная передача данных**: 
   - Процесс 0 отправляет данные процессу 1 через MPI_Send
   - Процесс i (1 <= i < N-1) получает данные от процесса i-1 через MPI_Recv и отправляет процессу i+1 через MPI_Send
   - Процесс N-1 только получает данные

### Роли процессов

- **Процесс 0**: источник данных, отправляет данные процессу 1
- **Промежуточные процессы (1..N-2)**: получают данные от левого соседа, передают правому
- **Процесс N-1**: конечный получатель, только принимает данные

## 6. Детали реализации

### Структура кода

Реализация разделена на следующие файлы:
- mpi/include/ops_mpi.hpp - заголовочный файл класса VdovinATopologyRulerMPI
- mpi/src/ops_mpi.cpp - реализация параллельного алгоритма
- seq/include/ops_seq.hpp - заголовочный файл последовательной версии
- seq/src/ops_seq.cpp - реализация последовательного алгоритма
- common/include/common.hpp - общие типы данных

### Ключевые методы

- ValidationImpl(): проверка корректности входных данных 
- PreProcessingImpl(): подготовка данных на процессе 0
- RunImpl(): основная логика передачи данных по топологии линейка
- PostProcessingImpl(): финальная обработка 

### Особые случаи

- **Один процесс**: данные копируются без передачи
- **Пустой вход**: валидация возвращает false
- **Разные размеры данных**: размер передается через MPI_Bcast перед передачей самих данных

### Использование памяти

Каждый процесс хранит копию всех данных в векторе data_. Для больших объемов данных это может быть ограничением, но обеспечивает простоту реализации.

## 7. Экспериментальная среда

| Компонент  | Значение                                |
| ---------- | --------------------------------------- |
| CPU        | Apple M2 (8 cores)                      |
| RAM        | 16 GB                                   |
| ОС         | OS: Ubuntu 24.04 (DevContainer / Mac)   |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release |
| MPI        | mpirun (Open MPI) 4.1.6                 |


### Окружение

- Переменные окружения: PPC_NUM_PROC для указания количества процессов
- Тестовые данные: вектор из 60 миллионов целых чисел для тестов производительности

### Генерация тестовых данных

Тестовые данные генерируются программно: вектор заполняется последовательными значениями от 0 до N-1, где N - размер вектора.

## 8. Результаты и анализ

### 8.1 Корректность

Корректность реализации проверяется с помощью функциональных тестов:

1. **Базовые тесты**: проверка работы на векторах размером 10, 100, 1000 элементов
2. **Граничные случаи**:
   - Один элемент
   - Два элемента
   - Пустой вход (валидация должна вернуть false)
   - Отрицательные значения
   - Большие данные (5000 элементов)
   - Все одинаковые значения
   - Убывающая последовательность
   - Чередующиеся положительные и отрицательные значения

Все тесты проходят, что в свою очередь подтверждает корректность реализации.

### 8.2 Производительность

Результаты замеров производительности представлены в таблицах ниже. 

#### Таблица 1: task_run

| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  | 1     | 0.0294  | 1.00    | N/A        |
| mpi  | 1     | 0.0289  | 1.02    | 101.7%     |
| mpi  | 2     | 0.1157  | 0.25    | 12.7%      |
| mpi  | 3     | 0.1938  | 0.15    | 5.1%       |
| mpi  | 4     | 0.2209  | 0.13    | 3.3%       |
| mpi  | 5     | 0.2323  | 0.13    | 2.5%       |

#### Таблица 2: pipeline

| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  | 1     | 0.0653  | 1.00    | N/A        |
| mpi  | 1     | 0.0450  | 1.45    | 145.2%     |
| mpi  | 2     | 0.1973  | 0.33    | 16.6%      |
| mpi  | 3     | 0.3215  | 0.20    | 6.8%       |
| mpi  | 4     | 0.2832  | 0.23    | 5.8%       |
| mpi  | 5     | 0.5694  | 0.11    | 2.3%       |

### Анализ результатов

Наблюдается следующая ситуация: при увеличении количества процессов время выполнения увеличивается, а не уменьшается. Это объясняется следующими факторами:

1. **Последовательная передача данных**: данные передаются последовательно по цепочке процессов, что создает узкое место. Каждый процесс должен дождаться получения данных от предыдущего процесса перед отправкой следующему.

2. **Накладные расходы на коммуникации**: каждая операция MPI_Send/MPI_Recv имеет накладные расходы, которые накапливаются при увеличении количества процессов.

Эффективность резко падает с увеличением количества процессов, что является ожидаемым поведением для последовательной передачи данных в линейной топологии.

## 9. Выводы

В ходе работы мной была реализована виртуальная топология "линейка" с использованием MPI. Алгоритм обеспечивает передачу данных от любого процесса любому другому в рамках линейной топологии.

**Достижения:**
- Корректная реализация топологии линейка
- Успешная передача данных между процессами
- Все функциональные тесты проходят

**Ограничения:**
- Производительность ухудшается с увеличением количества процессов из-за последовательной природы передачи данных
- Топология линейка не оптимальна для задачи широковещательной передачи данных
- Низкая эффективность использования вычислительных ресурсов

## 10. Источники

1. Сысоев А. В.: Курс лекций по алгоритмам и структурам данных, ННГУ, 2024 год.
2. MPI Forum. MPI: A Message-Passing Interface Standard. Version 4.0. https://www.mpi-forum.org/docs/
3. Сысоев А. В.: Курс лекций по параллельному программированию, ННГУ, 2025 год.
4. Стивенс Р.: "Алгоримы. Теория и практическое применение. 2-е издание." ЭКСМО, 2021 год.

## Приложение

### Фрагмент кода реализации RunImpl

```cpp
bool VdovinATopologyRulerMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int data_size = 0;

  if (rank == 0) {
    data_size = static_cast<int>(data_.size());
  }

  MPI_Bcast(&data_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    data_.resize(data_size);
  }

  if (size == 1) {
    GetOutput() = data_;
    return true;
  }

  if (rank == 0) {
    MPI_Send(data_.data(), data_size, MPI_INT, 1, 0, MPI_COMM_WORLD);
  } else {
    int left_neighbor = rank - 1;
    MPI_Recv(data_.data(), data_size, MPI_INT, left_neighbor, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    if (rank < size - 1) {
      int right_neighbor = rank + 1;
      MPI_Send(data_.data(), data_size, MPI_INT, right_neighbor, 0, MPI_COMM_WORLD);
    }
  }

  GetOutput() = data_;
  return true;
}
```

