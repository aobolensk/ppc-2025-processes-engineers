# Поразрядная сортировка для целых чисел с простым слиянием

- Студент: Постернак Алексей Николаевич, группа 3823Б1ПР2
- Технологии: SEQ | MPI
- Вариант: 18

## 1. Введение

Алгоритмы сортировок активно применяются в современных системах и технологиях. Они активно используются в программировании, больших данных, таблицах и т.д.

В процессе развития технологий, было придумано большое количество разных алгоритмов сортировок для различных задач, чтобы упорядочивание данных было максимально быстрым и эффективным. 

В данной работе мы разберем поразрядную сортировку для целых чисел, которая максимально эффективна на бошльших объемах данных.

**Цель:**
Разработать параллельную (MPI) реализацию алгоритма поразрядной сортировки для целых чисел с использованием простого слияния и сравнить производительность с последовательным (SEQ) алгоритмом поразрядной сортировки на различных объемах данных.

## 2. Постановка задачи

**Задача:**
Требуется отсортировать в порядке возрастания целые числа в `std::vector<int> input` используя алгоритм поразрядной сортировки.

**Ограничения:**
- Массив должен быть непустым
- Корректная обработка отрицательных чисел
- Гарантируется, что тип значений в векторе - `int`
- Результаты последовательного и параллельного алгоритма должны быть идентичными

## 3. Базовый алгоритм (последовательный)

**Входные данные:**
Массив целых значений (`std::vector<int> &input`).

**Выходные данные:**
Отсортированный по возрастанию массив целых значений (`std::vector<int> sorted_output`).

**Реализация алгоритма:**

Поразрядная сортировка (Radix sort) использует сортировку значений по разрядам.

В данном алгоритме реализуется последовательная поразрядная сортировка с младшего разряда (LSD). Работаем с байтами (типами из библиотеки `<cstdint>`) и битовыми операциями для определения разрядов и учета отрицательных чисел, так как операции деления и взятия остатка достаточно долгие и значительно ухудшают эффективность поразрядной соритровки.

1. Чтобы учесть отрицательные значения, `int32_t` перобразуется в `uint32_t` с помощью `XOR` с маской `0x80000000U`. В этом случае 31 бит инвертируется.

***Пояснение:*** После применения этого трюка, все отрицательные числа отображаются в диапазон `[0, 2^31)`, а неотрицательные - в диапозон `[2^31, 2^32)`. Таким образом внутри каждого диапозона сохранится правильный порядок. Обратное значение (тот же `XOR` с маской `0x80000000U`) восстановит исходные значения в отсортированном порядке после применения алгоритма поразрядной сортировки.

2. Инициализация буфера и констант для дальнейшей сортировки.
  ```cpp
  std::vector<uint32_t> buffer(n); // временный массив для распределения элементов
  constexpr int kNumBytes = 4; // в 32 битном числе 4 байта (т.е. 4 итерации, где каждый проход - один разряд в системе счисления с основанием 256)
  constexpr uint32_t kByteMask = 0xFFU; // 255 в десятичной системе, используется для извлечения одного байта
  ```
3. После ранее сделанных подготовок, сортируется массив:
  - Проход по 4 байтам, где первый байт - младший, а четвертый - старший
  - Создается массив count из 256 элементов (количество возможных значений байта), который хранит количество чисел, у которых текущий байт равен i-тому значению
  - Массив count после второго цикла хранит количество чисел, у которых текущий байт меньше или равен i-тому значению
  - Обработка каждого элемента массива в обратном порядке (используется для устойчивости), где каждый элемент помещается в буфер по индексу
4. Обратное преобразование `uint32_t` в `int32_t`.

***Сложность алгоритма по времени:*** `O(k * n), k = 4`

***Сложность алгоритма по памяти:*** `O(n)`

Полная реализация алгоритма находится в Приложении.

## 4. Схема распараллеливания

**Топология:**
В представленном алгоритме ценрализованная master-worker (звезднообразная) топология.

**Распределение данных:**
1. 0-процесс получает исходный массив.
2. Расылка размера исходного массива всем процессам с помощью `MPI_Bcast`.
3. Функция `CalculateCountsAndOffsets()` определяет, какую часть получит процесс и позицию своей части. Остаток получает 0-процесс.
4. Распределение данных с помощью `MPI_Scatterv`
5. Каждый процесс поразрядно сортирует свою часть в функции `RadixSortLocal()`. Принцип ее работы описан в последовательном алгоритме.
6. Получение отсортированных частей 0-процессом с помощью `MPI_Recv` и отправка результата сортировки других процессов нулевому с помощью `MPI_Send`.
7. Простое слияние отсортированных частей функцией `MergeSortedParts()` с использованием std::merge из библиотеки `<algorithm>`.
8. Рассылка результата всем процессам.

Полная реализация алгоритма находится в Приложении.

## 5. Детали реализации

**Структура проекта:**
```
- posternak_a_radix_merge_sort // корень проекта
    - common/include/common.hpp                 // определение типов входных и выходных данных
    - mpi                                       // реализация параллельного алгоритма
        - include/ops_mpi.hpp                   // объявление функций
        - src/ops_mpi.cpp                       // реализация функций
    - seq                                       // реализация последовательного алгоритма
        - include/ops_mpi.hpp                   // объявление функций
        - src/ops_mpi.cpp                       // реализация функций
    - test                                      // тестирование алгоритмов mpi и seq
        - functional/main.cpp                   // функциональные тесты
        - perfomance/main.cpp                   // тесты на производительность
    - info.json                                 // информация о студенте
    - report.md                                 // отчет
    - settings.json                             // настройки проекта
```

**Ключевые классы:**
- `PosternakARadixMergeSortSEQ` - последовательная реализация алгоритма
- `PosternakARadixMergeSortMPI` - параллельная реализация алгоритма

**Ключевые функции:**
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - предварительные вычисления  
- `RunImpl()` - реализация SEQ/MPI алгоритма
- `PostProcessingImpl()` - завершающая обработка

**Частные случаи:**
Предварительное условие - массив должен быть непустым, его значения - тип `int`

## 6. Экспериментальное окружение

**Аппаратное обеспечение:**
- Процессор: Intel Core i7-11800H @ 2.30GHz
- Ядра: 16 шт.
- ОЗУ: 16 ГБ
- ОС: Kubuntu 25.10

**Программный инструментарий:**
- Компилятор: g++ 15.2.0
- Тип сборки: Release
- Стандарт C++: C++23
- MPI: Open MPI 5.0.8

**Тестовое окружение**
```bash
PPC_NUM_PROC=1,2,4
```

## 7. Результаты

### 7.1 Корректность

Все функциональные тесты были успешно пройдены.

**Ключевые аспекты функцональных тестов:**

- Корректная сортировка отрицательных чисел
- Стабильность сортировки
- Корректная работа с граничными значениями (`INT_MAX` и `INT_MIN`)
- Корректная сортировка с одинаковыми значениями
- Тестировка алгоритма на отсортированных массивах
- Корректность при различном количестве элементов

SEQ и MPI версии выдают идентичные результаты для всех тестовых случаев.

### 7.2 Производительность

**Результаты замера времени выполнения MPI и SEQ алгоритмов для массива, длинной 10.000.000 значений:**

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
|-------|----------------------|----------|-----------|---------------|
| SEQ   | 1                    | 0.400    | 1.00      | N/A           |
| MPI   | 1                    | 0.484    | 0.83      | 83%           |
| MPI   | 2                    | 0.332    | 1.20      | 60%           |
| MPI   | 4                    | 0.290    | 1.38      | 35%           |
| MPI   | 8                    | 0.305    | 1.31      | 16%           |


**Формула ускорения:** Ускорение = Время SEQ / Время MPI

**Формула эффективности:** Эффективность = (Ускорение / Количество процессов) × 100%

### 7.3. Анализ результатов

**Лучшее ускорение:** 1.38 на 4 процессах

**Эффективность:** При увеличении числа процессов снижается показатель эффективности

## 8. Выводы

В результате выполнения проекта были разработаны и протестированы последовательный (SEQ) и параллельный (MPI) алгоритма поразрядной сортировки для целых чисел с простым слиянием.

Параллельный алгоритм показывает наибольшее ускорение относительно последовательного на 2 и 4 процессах. На 8 процессах эффективность значительно упала. Ускорение незначительное. Это связанно в первую очередь с простым слиянием отсортированных частей в параллельном алгоритме на 0-процессе

Итого, для больших объемов данных при использловании параллельного (MPI) алгоритма было получено максимальное ускорение 1.38 на 4 процессах относительно последовательного (SEQ) алгоритма.

## 9. Литература

1. Документация по курсу: "Параллельное программирование": https://learning-process.github.io/parallel_programming_course/ru/index.html (Оболенский А.А, Нестеров А.Ю)
2. Лекции по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Документация по MPI: https://www.open-mpi.org/

## Приложение

`ops_seq.cpp:`

```cpp
bool PosternakARadixMergeSortSEQ::RunImpl() {
  const std::vector<int> &input = GetInput();
  const auto n = static_cast<int>(input.size());

  // Перевод числа в unsigned int для учета отрицательных чисел
  std::vector<uint32_t> unsigned_data(static_cast<size_t>(n));
  for (int i = 0; i < n; i++) {
    unsigned_data[static_cast<size_t>(i)] = static_cast<uint32_t>(input[static_cast<size_t>(i)]) ^ 0x80000000U;
  }

  // Временный массив для распределения элементов
  std::vector<uint32_t> buffer(static_cast<size_t>(n));

  constexpr int kNumBytes = 4; // 4 байта в 32-битном числе
  constexpr uint32_t kByteMask = 0xFFU; // Маска для получения нужного байта

  for (int byte_index = 0; byte_index < kNumBytes; byte_index++) {
    // Количество значений в байте
    std::vector<int> count(256, 0);

    // Заполнение count
    for (uint32_t value : unsigned_data) {
      const auto current_byte = static_cast<uint8_t>((value >> (byte_index * 8)) & kByteMask);
      ++count[current_byte];
    }

    for (int i = 1; i < 256; i++) {
      count[i] += count[i - 1];
    }

    // Помещаем count в буфер
    // Идем в обратную сторону для устойчивости
    for (int i = n - 1; i >= 0; i--) {
      const auto current_byte =
          static_cast<uint8_t>((unsigned_data[static_cast<size_t>(i)] >> (byte_index * 8)) & kByteMask);
      buffer[static_cast<size_t>(--count[current_byte])] = unsigned_data[static_cast<size_t>(i)];
    }

    unsigned_data.swap(buffer);
  }

  std::vector<int> sorted_output(static_cast<size_t>(n));
  // Обратное преобразование в signed int
  for (int i = 0; i < n; i++) {
    sorted_output[static_cast<size_t>(i)] = static_cast<int>(unsigned_data[static_cast<size_t>(i)] ^ 0x80000000U);
  }

  GetOutput() = std::move(sorted_output);
  return true;
}
```

`ops_mpi.cpp:`

```cpp
bool PosternakARadixMergeSortMPI::RunImpl() {
  int size = 0;
  int rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  int input_len = 0;
  std::vector<int> *input = nullptr;

  // Процесс-0 получает исходный массив
  if (rank == 0) {
    input = &GetInput();
    input_len = static_cast<int>(input->size());
  }

  // Рассылка размера массива всем процессам
  MPI_Bcast(&input_len, 1, MPI_INT, 0, MPI_COMM_WORLD);
  // Для устранения ошибок компилятора добавим проверку
  if (input_len == 0) {
    GetOutput() = std::vector<int>();
    return true;
  }

  // Определение размера части массива и его расположения в памяти для каждого процесса
  std::vector<int> counts;
  std::vector<int> offset;
  CalculateCountsAndOffsets(input_len, size, counts, offset);

  const int local_size = counts[rank];
  std::vector<int> input_local(local_size);

  // Рассылка кусков массива всем процессам
  if (rank == 0) {
    MPI_Scatterv(input->data(), counts.data(), offset.data(), MPI_INT, input_local.data(), local_size, MPI_INT, 0,
                 MPI_COMM_WORLD);
  } else {
    MPI_Scatterv(nullptr, counts.data(), offset.data(), MPI_INT, input_local.data(), local_size, MPI_INT, 0,
                 MPI_COMM_WORLD);
  }

  // Поразрядная сортировка каждым процессом
  std::vector<uint32_t> unsigned_sorted = RadixSortLocal(input_local);
  // Обратное преобразование в signed int
  std::vector<int> local_sorted = ConvertToSigned(unsigned_sorted);

  std::vector<int> result;
  // Получение результатов поразрядной сортировки процессов
  if (rank == 0) {
    std::vector<std::vector<int>> sorted_proc_parts;
    sorted_proc_parts.reserve(static_cast<size_t>(size));
    sorted_proc_parts.push_back(std::move(local_sorted));

    for (int proc = 1; proc < size; proc++) {
      std::vector<int> remote_sorted_proc_part(counts[proc]);
      MPI_Recv(remote_sorted_proc_part.data(), counts[proc], MPI_INT, proc, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      sorted_proc_parts.push_back(std::move(remote_sorted_proc_part));
    }

    result = std::move(sorted_proc_parts[0]);
    // Простое слияние всех отсортированных частей
    MergeSortedParts(result, sorted_proc_parts);
  } else {
    MPI_Send(local_sorted.data(), local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);
  }

  // Сохранение результата
  std::vector<int> output(input_len);
  if (rank == 0) {
    output = std::move(result);
  }

  // Рассылка результата всем процессам
  MPI_Bcast(output.data(), input_len, MPI_INT, 0, MPI_COMM_WORLD);
  GetOutput() = std::move(output);

  return true;
}

```