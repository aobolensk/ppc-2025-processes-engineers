# Метод Гаусса – ленточная горизонтальная схема

- **Студент**: Шилин Никита Дмитриевич, группа 3823Б1ПР1
- **Технологии**: SEQ | MPI
- **Вариант**: 2

## 1. Введение

Метод Гаусса для решения систем линейных уравнений - один из фундаментальных алгоритмов вычислительной математики. Для больших систем уравнений важна эффективная параллельная реализация, особенно для ленточных матриц, которые часто встречаются в практических задачах.

## 2. Постановка задачи

**Цель работы:** реализовать метод Гаусса для решения систем линейных уравнений с ленточными матрицами, используя последовательный подход (SEQ) и параллельный подход (MPI) с горизонтальной схемой распределения данных.

**Формальная постановка:**

Для системы линейных уравнений `Ax = b`, где:
- `A` - ленточная матрица размера `n × n`
- `b` - вектор правой части размера `n`
- `x` - вектор неизвестных размера `n`

Требуется найти решение `x`.

**Входные данные:**
- Расширенная матрица `[A|b]` размера `n × (n+1)` (тип `std::vector<std::vector<double>>`)
- Матрица `A` является ленточной (большинство элементов вне ленты равны нулю)

**Выходные данные:**
- Вектор решения `x` размера `n` (тип `std::vector<double>`)

## 3. Базовый алгоритм (последовательная версия)

### 3.1. Этапы выполнения задачи

**1. Валидация данных (`ValidationImpl`):**
- Проверка корректности размеров матрицы
- Проверка, что матрица не пустая
- Проверка, что все строки имеют одинаковую длину

**2. Предобработка данных (`PreProcessingImpl`):**
- Инициализация выходного вектора

**3. Вычисления (`RunImpl`):**
- Прямой ход метода Гаусса (приведение к верхнетреугольному виду)
- Обратный ход (back substitution) для нахождения решения

**4. Постобработка данных (`PostProcessingImpl`):**
- Проверка корректности результата

### 3.2. Алгоритм последовательной реализации

**Псевдокод:**

```
function RunImpl():
    augmented_matrix = GetInput()
    n = augmented_matrix.size()
    cols = augmented_matrix[0].size()
    
    // Прямой ход
    for k = 0 to n - 1:
        // Поиск ведущего элемента
        max_row = k
        max_val = |augmented_matrix[k][k]|
        for i = k + 1 to n - 1:
            if |augmented_matrix[i][k]| > max_val:
                max_val = |augmented_matrix[i][k]|
                max_row = i
        
        // Перестановка строк
        swap(augmented_matrix[k], augmented_matrix[max_row])
        
        // Исключение
        for i = k + 1 to n - 1:
            factor = augmented_matrix[i][k] / augmented_matrix[k][k]
            for j = k to cols - 1:
                augmented_matrix[i][j] -= factor * augmented_matrix[k][j]
    
    // Обратный ход
    x = vector of size n
    for i = n - 1 down to 0:
        sum = 0
        for j = i + 1 to n - 1:
            sum += augmented_matrix[i][j] * x[j]
        x[i] = (augmented_matrix[i][cols-1] - sum) / augmented_matrix[i][i]
    
    GetOutput() = x
    return true
```

### 3.3. Сложность алгоритма

| Параметр                  | Значение |
|---------------------------|----------|
| Временная сложность       | O(n³)    |
| Пространственная сложность| O(n²)    |

где `n` - размер матрицы.

## 4. Схема параллельного алгоритма

Схема параллельного алгоритма использует **горизонтальное распределение данных**: строки матрицы распределяются между процессами по принципу round-robin (строка `i` обрабатывается процессом `i % size`).

### 4.1. Распределение данных

**Алгоритм распределения:**

1. **Инициализация:**
   - Все процессы запускаются в коммуникаторе `MPI_COMM_WORLD`
   - Каждый процесс получает свой ранг (`rank`) и общее количество процессов (`size`)

2. **Распределение строк:**
   - Строка `i` матрицы обрабатывается процессом `i % size`
   - Каждый процесс получает примерно `n / size` строк

3. **Передача данных:**
   - Процесс 0 отправляет строки соответствующим процессам через `MPI_Send`
   - Остальные процессы получают свои строки через `MPI_Recv`

### 4.2. Топология коммуникаций

- **Топология:** все процессы связаны через `MPI_COMM_WORLD`
- **Роль процессов:**
  - **Процесс 0:** координирует работу, распределяет данные, собирает результаты
  - **Процессы 1..N-1:** обрабатывают свои строки, участвуют в вычислениях

### 4.3. Паттерны коммуникации

**Используемые MPI операции:**

1. **MPI_Send/MPI_Recv** - распределение строк матрицы:
   - Тип операции: Point-to-Point
   - Режим работы: блокирующая операция

2. **MPI_Bcast** - рассылка ведущей строки:
   ```cpp
   MPI_Bcast(pivot_row.data(), cols, MPI_DOUBLE, owner_process, MPI_COMM_WORLD);
   ```
   - Тип операции: One-to-All
   - Режим работы: блокирующая операция

3. **MPI_Bcast** - рассылка вычисленных значений в обратном ходе:
   ```cpp
   MPI_Bcast(&x[i], 1, MPI_DOUBLE, owner_process, MPI_COMM_WORLD);
   ```

### 4.4. Распределение вычислений

1. **Прямой ход:**
   - Для каждой ведущей строки `k`:
     - Процесс `k % size` отправляет строку всем процессам через `MPI_Bcast`
     - Все процессы обновляют свои локальные строки

2. **Обратный ход:**
   - Для каждого `i` от `n-1` до `0`:
     - Процесс `i % size` вычисляет `x[i]` и рассылает всем процессам

## 5. Детали реализации

### 5.1. Структура кода

**Файловая структура:**

```
shilin_n_gauss_band_horizontal_scheme/
├── common/
│   └── include/common.hpp          - определения типов данных
├── seq/
│   ├── include/ops_seq.hpp         - заголовочный файл последовательной версии
│   └── src/ops_seq.cpp             - исходный код последовательной версии
├── mpi/
│   ├── include/ops_mpi.hpp          - заголовочный файл MPI-реализации
│   └── src/ops_mpi.cpp             - исходный код параллельной версии
└── tests/
    ├── functional/
    │   └── main.cpp                - функциональные тесты
    └── performance/
        └── main.cpp                - тесты производительности
```

**Ключевые классы реализации:**

- `ShilinNGaussBandHorizontalSchemeSEQ` - класс последовательной реализации
- `ShilinNGaussBandHorizontalSchemeMPI` - класс параллельной реализации

## 6. Экспериментальная установка

### 6.1. Аппаратное обеспечение

| Параметр | Значение                                            |
|----------|-----------------------------------------------------|
| CPU      | Apple M4 Pro (10 performance cores + 4 efficiency cores) |
| RAM      | 24 GB                                               |
| OS       | macOS Tahoe 26.0                                    |

### 6.2. Программное обеспечение

| Параметр   | Значение                    |
|------------|----------------------------|
| Компилятор | AppleClang 17.0.0.17000013 |
| MPI        | Open MPI 5.0.8             |
| Сборка     | Release                    |
| CMake      | 3.27.3                      |

## 7. Результаты и обсуждение

### 7.1. Корректность

Корректность реализации проверена функциональными тестами на различных размерах матриц (от 3×3 до 10×10) с различной шириной ленты. Все тесты пройдены успешно.

### 7.2. Производительность

Реализация протестирована на матрицах размера 1000×1000. Такой размер выбран, чтобы время выполнения было больше 0.001 с и его было корректно анализировать. Ниже приведены измеренные значения времени выполнения (в секундах) для режимов `pipeline` и `task_run`.

| Число процессов | MPI pipeline, s | MPI task_run, s | SEQ pipeline, s | SEQ task_run, s |
|----------------|----------------:|----------------:|----------------:|----------------:|
| 1              | 0.1422498000     | 0.1425424000     | 0.0019756250     | 0.0021268416     |
| 2              | 0.0811784000     | 0.0800184000     | 0.0019756250     | 0.0021268416     |
| 4              | 0.0442430000     | 0.0422068000     | 0.0019756250     | 0.0021268416     |

## 8. Выводы

1. Успешно реализованы последовательная и параллельная версии метода Гаусса
2. Горизонтальная схема распределения данных обеспечивает эффективное распараллеливание
3. Результаты SEQ и MPI версий совпадают для всех тестовых случаев

## 9. Источники

1. Сысоев А. В. "Коллективные и парные взаимодействия" // Лекции по дисциплине «Параллельное программирование для кластерных систем». — 2025.

2. Документация по курсу «Параллельное программирование» // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html

3. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.0. 2021. URL: https://www.mpi-forum.org/docs/

