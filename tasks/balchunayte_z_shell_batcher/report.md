# Сортировка Шелла с чётно-нечётным слиянием Бэтчера

- Студент: Бальчунайте Злата Денисовна, группа 3823Б1ПР3  
- Технология: MPI + SEQ  
- Вариант: 17  

## 1. Введение
Сортировка — одна из ключевых задач параллельного программирования, лежащая в основе обработки больших массивов данных, численных методов и анализа информации.  
Для эффективного распараллеливания сортировки часто используются сети сравнения, обеспечивающие детерминированную структуру обменов между процессами.

Цель данной работы — реализовать алгоритм сортировки Шелла с использованием чётно-нечётного слияния Бэтчера (odd-even merge), разработать последовательную и MPI-реализации, интегрировать их в инфраструктуру PPC и сравнить корректность и производительность решений.

## 2. Постановка задачи
Дан массив целых чисел:

\[
a = (a_1, a_2, \ldots, a_n)
\]

Необходимо получить отсортированный массив:

\[
a' = sort(a)
\]

### Входные и выходные данные
- Input: вектор `std::vector<int>`
- Output: отсортированный вектор `std::vector<int>`

### Требования
- Реализация должна поддерживать SEQ и MPI версии.
- MPI-алгоритм должен использовать сортировку Шелла локально и слияние Бэтчера для объединения отсортированных частей.
- Решение должно корректно работать в инфраструктуре PPC и проходить функциональные и перф-тесты.

## 3. Базовый алгоритм (последовательный)
Последовательная версия состоит из двух этапов:

1. Разбиение исходного массива на несколько блоков.
2. Сортировка каждого блока алгоритмом Shell Sort.
3. Попарное слияние отсортированных блоков с помощью чётно-нечётного слияния Бэтчера, пока не останется один отсортированный массив.

Shell Sort выбран как простой и эффективный алгоритм для локальной сортировки средних массивов, а слияние Бэтчера — как детерминированная сеть сравнения, удобная для параллельных вычислений.

## 4. Схема распараллеливания (MPI)

### Распределение данных
1. Процесс 0 получает входной массив.
2. Размер массива рассылается всем процессам с помощью `MPI_Bcast`.
3. Формируются массивы:
   - `counts[rank]` — количество элементов на каждом процессе;
   - `displs[rank]` — смещения в исходном массиве.
4. Данные распределяются между процессами с помощью `MPI_Scatterv`.

### Локальная сортировка
Каждый процесс выполняет сортировку Шелла своего локального подмассива.

### Глобальное слияние (Batcher odd-even merge)
Слияние выполняется по схеме двоичного дерева:

- На шаге `step = 1, 2, 4, ...`
  - если `rank % (2 * step) == 0`, процесс принимает данные от соседа `rank + step` и выполняет слияние Бэтчера;
  - иначе процесс отправляет свой отсортированный массив процессу `rank - step` и завершает работу.

Для корректности слияния массивы дополняются padding-элементами до длины, равной степени двойки.

### Распространение результата
PPC-инфраструктура требует, чтобы результат был доступен на всех процессах, поэтому итоговый отсортированный массив, полученный на процессе 0, рассылается всем процессам с помощью `MPI_Bcast`.

## 5. Детали реализации

### Структура проекта
tasks/balchunayte_z_shell_batcher/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
└── tests/

### Ключевые компоненты
- `BalchunayteZShellBatcherSEQ`:
  - сортировка массива в одном процессе;
  - разбиение на блоки, Shell Sort и слияние Бэтчера.
- `BalchunayteZShellBatcherMPI`:
  - распределение данных (`Scatterv`);
  - локальная сортировка Шелла;
  - иерархическое слияние через odd-even merge;
  - рассылка результата.

Рекурсия в сетях слияния не используется — все алгоритмы реализованы итеративно для соответствия требованиям `clang-tidy`.

## 6. Экспериментальная установка

### Аппаратное и программное окружение
- **CPU:** 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz  
- **Ядер / потоков:** 4 / 8  
- **ОЗУ:** 8 ГБ  
- **ОС:** Windows 10 22H2  

### Инструменты
- **Компилятор:** MSVC  
- **Форматирование и анализ:** clang-format, clang-tidy  
- **MPI:** Microsoft MPI (MS-MPI)  
- **Сборка:** Release  

### Команды запуска
- Последовательная версия:
`.\ppc_perf_tests.exe --gtest_filter=RunModeTests/ShellBatcherRunPerfTestProcesses.*`
- MPI-версия:
`mpiexec -n <k> .\ppc_perf_tests.exe --gtest_filter=RunModeTests/ShellBatcherRunPerfTestProcesses.*`

### Данные
Входные данные в тестах генерируются детерминированно внутри тестового фреймворка PPC.

## 7. Результаты и обсуждение

### 7.1 Корректность
Корректность подтверждена следующими фактами:
- все функциональные тесты SEQ проходят;
- все функциональные тесты MPI проходят при `mpiexec -n 2` и `mpiexec -n 4`;
- результаты SEQ и MPI полностью совпадают;
- корректно обрабатываются пустые массивы, массивы из одного элемента, уже отсортированные, обратные и с дубликатами.

### 7.2 Производительность
Замеры выполнены с использованием перф-тестов PPC:

- SEQ:
  `.\ppc_perf_tests.exe --gtest_filter=RunModeTests/ShellBatcherRunPerfTestProcesses.*`
- MPI:
  `mpiexec -n <k> .\ppc_perf_tests.exe --gtest_filter=RunModeTests/ShellBatcherRunPerfTestProcesses.*`

Размер входного массива в перф-тесте: **200000** элементов (детерминированная генерация).

Ниже приведены результаты для `pipeline` и `task_run`. Для расчёта ускорения в качестве базового времени используется **SEQ pipeline** из того же запуска (чтобы минимизировать влияние фоновой нагрузки ОС).

#### Запуск без mpiexec (SEQ + MPI в 1 процессе)
| Реализация | Режим | Время (с) |
|-----------|------|-----------|
| MPI | pipeline | 0.1328466800 |
| MPI | task_run | 0.0478721000 |
| SEQ | pipeline | 1.5200723000 |
| SEQ | task_run | 1.4673724200 |

#### MPI: 2 процесса
| Реализация | Режим | Время (с) |
|-----------|------|-----------|
| MPI | pipeline | 0.5475339401 |
| MPI | task_run | 1.4772691000 |
| SEQ | pipeline | 1.5481454000 |
| SEQ | task_run | 1.4941946000 |

#### MPI: 4 процесса
| Реализация | Режим | Время (с) |
|-----------|------|-----------|
| MPI | pipeline | 0.6872504800 |
| MPI | task_run | 4.4442941800 |
| SEQ | pipeline | 1.6452882000 |
| SEQ | task_run | 1.5628724400 |

#### Ускорение и эффективность (по pipeline)

| Процессы | SEQ pipeline (с) | MPI pipeline (с) | Ускорение | Эффективность |
|----------|------------------:|-----------------:|----------:|--------------:|
| 2 | 1.5481454000 | 0.5475339401 | 2.83 | 141% |
| 4 | 1.6452882000 | 0.6872504800 | 2.39 | 60% |


## 8. Заключение
В ходе работы:

- реализована сортировка Шелла с чётно-нечётным слиянием Бэтчера;
- разработаны последовательная и MPI-реализации;
- алгоритм корректно интегрирован в инфраструктуру PPC;
- все функциональные и перф-тесты успешно пройдены;
- код приведён в соответствие со стилем и проверен `clang-tidy`.

MPI-реализация демонстрирует корректность и масштабируемость, ограниченную накладными расходами на коммуникацию в условиях одного вычислительного узла.

## 9. Источники
1. Учебные материалы курса PPC  
2. Документация Microsoft MPI  

## Приложение (опционально)
```cpp
// Схема слияния по дереву
for (int step = 1; step < size; step <<= 1) {
if (rank % (2 * step) == 0) {
  receive and merge;
} else {
  send and exit;
}
}