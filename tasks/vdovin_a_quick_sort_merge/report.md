# Быстрая сортировка с простым слиянием

- **Студент**: Вдовин Артемий Николаевич
- **Группа**: 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: 14

## 1. Введение

Быстрая сортировка является одним из наиболее эффективных алгоритмов сортировки с временной сложностью O(n log n) в среднем случае. 

Цель работы: реализация параллельной версии быстрой сортировки с использованием технологии MPI и сравнению ее производительности с последовательной реализацией.

## 2. Постановка задачи

Требуется реализовать алгоритм быстрой сортировки с простым слиянием для сортировки массива целых чисел. Алгоритм должен быть реализован в двух вариантах: последовательном (SEQ) и параллельном (MPI). 

Входные данные: массив целых чисел произвольного размера.
Выходные данные: отсортированный по возрастанию массив целых чисел.

Ограничения:
- Размер входного массива не должен превышать 1000000 элементов
- Значения элементов должны находиться в диапазоне от -1000000 до 1000000

## 3. Базовый алгоритм 

Последовательный алгоритм состоит из следующих этапов:

1. Разделение массива на две части примерно равного размера
2. Рекурсивная сортировка каждой части с помощью алгоритма быстрой сортировки
3. Слияние двух отсортированных частей в один отсортированный массив

Алгоритм быстрой сортировки использует стратегию "разделяй и властвуй":
- Выбирается опорный элемент pivot из середины массива
- Элементы переставляются так, чтобы слева от pivot были элементы меньше или равные ему, а справа больше
- Рекурсивно сортируются левая и правая части

Алгоритм слияния объединяет два отсортированных массива в один, сравнивая элементы по порядку и выбирая меньший на каждой итерации.

## 4. Параллельный алгоритм

Параллельная реализация использует технологию MPI для распределения вычислений между несколькими процессами. Алгоритм состоит из следующих этапов:

1. Распределение данных: процесс с рангом 0 получает исходный массив и распределяет его между всеми процессами с помощью MPI_Scatterv. Размер массива делится на количество процессов, остаток распределяется между первыми процессами.

2. Локальная сортировка: каждый процесс получает свою часть массива и сортирует ее локально с помощью алгоритма быстрой сортировки.

3. Сбор результатов: процессы с рангом больше 0 отправляют отсортированные части процессу с рангом 0 с помощью MPI_Send.

4. Слияние: процесс 0 последовательно сливает полученные отсортированные части в один массив с помощью функции Merge.

5. Распространение результата: процесс 0 отправляет финальный отсортированный массив всем остальным процессам с помощью MPI_Send, а остальные процессы получают его с помощью MPI_Recv.

Использование MPI_Scatterv позволяет равномерно распределить данные между процессами, даже если размер массива не делится нацело на количество процессов. Этап слияния выполняется последовательно на процессе 0, что является узким местом при большом количестве процессов.

Схема распараллеливания включает следующие шаги коммуникации:

1. Распределение данных (MPI_Scatterv): процесс 0 распределяет исходный массив между всеми процессами
2. Локальная обработка: каждый процесс сортирует свою часть независимо
3. Сбор данных (MPI_Send/MPI_Recv): процессы отправляют отсортированные части процессу 0
4. Слияние: процесс 0 последовательно объединяет части
5. Распространение результата (MPI_Send/MPI_Recv): процесс 0 отправляет финальный массив всем процессам

Топология коммуникации представляет собой звезду с процессом 0 в центре. Все процессы обмениваются данными только с процессом 0, что упрощает реализацию, но создает узкое место при большом количестве процессов.

## 6. Детали реализации

Структура кода:
- common/include/common.hpp - общие типы данных (InType, OutType)
- seq/include/ops_seq.hpp, seq/src/ops_seq.cpp - последовательная реализация
- mpi/include/ops_mpi.hpp, mpi/src/ops_mpi.cpp - параллельная реализация MPI
- tests/functional/main.cpp - функциональные тесты
- tests/performance/main.cpp - тесты производительности

Главные функции:
- QuickSort - рекурсивная функция быстрой сортировки
- Merge - функция слияния двух отсортированных массивов
- ValidationImpl - проверка корректности входных данных
- PreProcessingImpl - подготовка данных
- RunImpl - основная логика сортировки
- PostProcessingImpl - проверка результата

## 7. Экспериментальная установка

|  Компонент |               Значение                       |
|------------|----------------------------------------------|
|     CPU    |           Apple M2 (8 cores)                 |
|     RAM    |                 16 GB                        |
|     ОС     | OS: Ubuntu 24.04 (DevContainer / macOs)      |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release      |
|     MPI    |        mpirun (Open MPI) 4.1.6               |

Параметры тестирования:
- Размер тестового массива: 1000000 элементов
- Количество процессов: 1, 2, 4, 6, 9
- Тестовые данные: массив целых чисел от 1 до 1000000 в обратном порядке

Генерация данных:
- Для функциональных тестов используется генератор случайных чисел с фиксированным seed для детерминированности
- Для тестов производительности используется массив размера 1000000 элементов

## 8. Результаты

### 7.1 Корректность

Корректность реализации проверяется следующими способами:

1. Функциональные тесты покрывают различные сценарии:
   - Пустой массив
   - Массив из одного элемента
   - Уже отсортированный массив
   - Массив в обратном порядке
   - Массив с дубликатами
   - Массив с отрицательными числами
   - Большой массив (127 элементов)
   - Сгенерированный массив (50000 элементов)
   - Различные комбинации 

2. Проверка результата включает:
   - Сравнение размера входного и выходного массивов
   - Проверка отсортированности результата
   - Проверка сохранения суммы элементов 

3. Все тесты проходят успешно как для последовательной, так и для параллельной реализации.

### 7.2 Производительность

Измерения проводились на массиве из 1000000 элементов. Их результат можно наблюдать в двух таблицах ниже:

#### task_run

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 0.0945   | 1.00      | N/A           |
| mpi   | 1         | 0.0582   | 1.62      | 162.0%        |
| mpi   | 2         | 0.0875   | 1.08      | 54.0%         |
| mpi   | 4         | 0.1251   | 0.76      | 19.0%         |
| mpi   | 6         | 0.1831   | 0.52      | 8.6%          |
| mpi   | 8         | 0.2739   | 0.35      | 4.4%          |

#### task_pipeline

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 0.1142   | 1.00      | N/A           |
| mpi   | 1         | 0.0722   | 1.58      | 158.0%        |
| mpi   | 2         | 0.1022   | 1.12      | 56.0%         |
| mpi   | 4         | 0.1495   | 0.76      | 19.0%         |
| mpi   | 6         | 0.2174   | 0.53      | 8.8%          |
| mpi   | 8         | 0.2984   | 0.38      | 4.8%          |

Анализ:

Анализ task_run:
- На 1 процессе MPI версия превосходит по ускорению последовательную версию (1.62)
- При увеличении количества процессов до 2 происходит незначительное ускорение (1.08), но эффективность падает до 54%
- При увеличении количества процессов (4, 6, 9) время выполнения увеличивается, а ускорение становится меньше 1, что указывает на преобладание накладных расходов на коммуникацию

Анализ task_pipeline:
- На 1 процессе MPI версия показывает ускорение 1.58 по сравнению с последовательной версией
- На 2 процессах ускорение составляет 1.12 с эффективностью 56%
- При увеличении количества процессов эффективность снижается, что связано с накладными расходами на коммуникацию и последовательным выполнением этапа слияния

Выводы:
- Параллельная реализация в данном случае показывает эффективность только на малом количестве процессов
- Накладные расходы на коммуникацию (через MPI_Scatterv, MPI_Send, MPI_Recv) становятся преобладающими при большом количестве процессов

## 9. Выводы

Мной была реализована параллельная версия алгоритма быстрой сортировки с простым слиянием с использованием технологии MPI, а так же реализация последовательного алгоритма. Реализация работает на разном количестве процессов и демонстрирует ускорение по сравнению с последовательной версией.

Что было достигнуто:
- Реализованы последовательная и параллельная версии алгоритма
- Функциональные тесты покрывают большую часть лабораторной работы
- Реализованы тесты производительности для разных режимов выполнения

Сложности:
- Накладные расходы на коммуникацию снижают эффективность при большом количестве процессов
- Для очень больших массивов может потребоваться оптимизация алгоритма слияния

## 10. Источники

1. Сысоев А. В.: Курс лекций по алгоритмам и структурам данных, ННГУ, 2024 год.
2. MPI Forum. MPI: A Message-Passing Interface Standard. Version 4.0. https://www.mpi-forum.org/docs/
3. Сысоев А. В.: Курс лекций по параллельному программированию, ННГУ, 2025 год.
4. Стивенс Р.: "Алгоримы. Теория и практическое применение. 2-е издание." ЭКСМО, 2021 год.

## Приложение

Основные функции сортировки:

```cpp
void QuickSort(std::vector<int> &arr, int left, int right) {
  if (left >= right) {
    return;
  }
  int pivot = arr[(left + right) / 2];
  int i = left;
  int j = right;
  while (i <= j) {
    while (arr[i] < pivot) {
      i++;
    }
    while (arr[j] > pivot) {
      j--;
    }
    if (i <= j) {
      std::swap(arr[i], arr[j]);
      i++;
      j--;
    }
  }
  QuickSort(arr, left, j);
  QuickSort(arr, i, right);
}

std::vector<int> Merge(const std::vector<int> &left, const std::vector<int> &right) {
  std::vector<int> result;
  result.reserve(left.size() + right.size());
  size_t i = 0;
  size_t j = 0;
  while (i < left.size() && j < right.size()) {
    if (left[i] <= right[j]) {
      result.push_back(left[i]);
      i++;
    } else {
      result.push_back(right[j]);
      j++;
    }
  }
  while (i < left.size()) {
    result.push_back(left[i]);
    i++;
  }
  while (j < right.size()) {
    result.push_back(right[j]);
    j++;
  }
  return result;
}
```

